[
  {
    "id": 1,
    "title": "Azure SQL Database: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to azure sql database, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"Azure SQL Database: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"Azure SQL Database\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to azure sql database, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n| Target Azure Service            | Migration Method                          | Type    | Description                                                                                   | Applicability (Azure SQL MI) | Applicability (Azure SQL DB) | Applicability (SQL Server on Azure VM) |\n|--------------------------------|-------------------------------------------|---------|-----------------------------------------------------------------------------------------------|------------------------------|------------------------------|----------------------------------------|\n| Azure SQL Managed Instance     | Managed Instance Link (Always On AG)      | Online  | Leverages Always On AG for near real-time replication with minimal downtime.                 | Yes                          | No                           | No                                     |\n| Azure SQL Managed Instance     | Log Replay Service (LRS)                  | Online  | Continuously restores log backups from Azure Blob Storage for near-zero downtime.            | Yes                          | No                           | No                                     |\n| Azure SQL Managed Instance     | Azure Database Migration Service (DMS)    | Online  | Fully managed service orchestrating backup/restore and log shipping for continuous sync.     | Yes                          | Limited                      | Yes                                    |\n| Azure SQL Managed Instance     | Transactional Replication                 | Online  | Configures Managed Instance as a subscriber for continuous data synchronization.             | Yes                          | Yes                          | Yes                                    |\n| Azure SQL Managed Instance     | Native Backup and Restore (.bak)         | Offline | Take full backup on-premises, upload to Azure Blob Storage, then restore to MI.              | Yes                          | No                           | Yes                                    |\n| Azure SQL Managed Instance     | Azure Database Migration Service (DMS)    | Offline | DMS orchestrates backup/restore; source is offline during migration.                         | Yes                          | Yes                          | Yes                                    |\n| Azure SQL Managed Instance     | BACPAC Import                             | Offline | Export schema and data to .bacpac, then import to MI. Best for smaller databases.            | Yes                          | Yes                          | No                                     |\n| Azure SQL Database             | Transactional Replication                 | Online  | Configure Azure SQL DB as a subscriber for continuous data synchronization.                  | Yes                          | Yes                          | Yes                                    |\n| Azure SQL Database             | Azure Database Migration Service (DMS)    | Online  | While DMS supports online, for Azure SQL DB it often means logical migration with downtime.  | Limited                      | Yes                          | Yes                                    |\n| Azure SQL Database             | BACPAC Export/Import                      | Offline | Export schema and data to .bacpac, then import to Azure SQL DB. Best for smaller databases.  | Yes                          | Yes                          | No                                     |\n| Azure SQL Database             | Generate Scripts (with data)             | Offline | Generate SQL scripts for schema and data, then execute on Azure SQL DB. For small databases. | No                           | Yes                          | No                                     |\n| Azure SQL Database             | BCP (Bulk Copy Program)                   | Offline | Export data from tables using BCP, then import to Azure SQL DB. Requires manual schema.      | No                           | Yes                          | No                                     |\n| Azure SQL Database             | Azure Data Factory (Initial Load)         | Offline | Create data pipelines for initial bulk data copy. Subsequent incremental loads can be online.| No                           | Yes                          | No                                     |\n| SQL Server on Azure VM         | Always On Availability Groups             | Online  | Extend on-premises Always On AG to include an Azure VM replica, then failover.               | No                           | No                           | Yes                                    |\n| SQL Server on Azure VM         | Transactional Replication                 | Online  | Configure Azure VM as a subscriber for continuous data synchronization.                      | Yes                          | Yes                          | Yes                                    |\n| SQL Server on Azure VM         | Log Shipping                              | Online  | Continuously back up transaction logs on-premises and restore to Azure VM.                   | No                           | No                           | Yes                                    |\n| SQL Server on Azure VM         | Azure Database Migration Service (DMS)    | Online  | DMS orchestrates backup/restore with log shipping for continuous sync to Azure VM.           | Yes                          | Limited                      | Yes                                    |\n| SQL Server on Azure VM         | Native Backup and Restore (.bak)         | Offline | Standard SQL Server backup/restore to/from Azure VM.                                         | Yes                          | No                           | Yes                                    |\n| SQL Server on Azure VM         | Detach and Attach Database Files          | Offline | Copy .mdf/.ldf files to Azure Blob Storage, then attach to SQL Server on Azure VM.           | No                           | No                           | Yes                                    |\n| SQL Server on Azure VM         | Azure Migrate (VM Lift-and-Shift)         | Offline | Migrates the entire on-premises server (OS, SQL Server, apps) as an Azure VM.                | No                           | No                           | Yes                                    |\n| SQL Server on Azure VM         | Azure Database Migration Service (DMS)    | Offline | DMS orchestrates backup/restore for offline migration to Azure VM.                           | Yes                          | Yes                          | Yes                                    |\n| SQL Server on Azure VM         | Data Box Family                           | Offline | Physically ship large datasets to Azure, then restore to SQL VM using native methods.        | No                           | No                           | Yes                                    |\n\n---\n| Migration Method                            | Type     | Description                                                                                   | Azure SQL MI | Azure SQL DB | SQL Server on Azure VM |\n|---------------------------------------------|----------|-----------------------------------------------------------------------------------------------|--------------|---------------|-------------------------|\n| **Always On AG / MI Link**                  | Online   | High-availability replication with minimal downtime.                                          | \u2705            | \u274c             | \u2705 (AG only)            |\n| **Log-based (LRS / Log Shipping)**          | Online   | Continuous restore of transaction logs for near-zero downtime.                               | \u2705 (LRS)      | \u274c             | \u2705 (Log Shipping)       |\n| **Transactional Replication**               | Online   | Continuous data sync by configuring as a subscriber.                                          | \u2705            | \u2705             | \u2705                      |\n| **Azure DMS (Online)**                      | Online   | Uses backup/restore + log shipping or logical sync.                                           | \u2705            | \u26a0\ufe0f (Logical)   | \u2705                      |\n| **Azure DMS (Offline)**                     | Offline  | Orchestrates full backup/restore, requires downtime.                                          | \u2705            | \u2705             | \u2705                      |\n| **Native Backup/Restore (.bak)**            | Offline  | Backup DB locally, upload to Blob, and restore.                                               | \u2705            | \u274c             | \u2705                      |\n| **BACPAC Import/Export**                    | Offline  | Export schema + data, best for small DBs.                                                     | \u2705            | \u2705             | \u274c                      |\n| **Generate Scripts / BCP**                  | Offline  | Manual scripting or data export/import, best for very small DBs.                             | \u274c            | \u2705             | \u274c                      |\n| **Azure Data Factory (Bulk Load)**          | Offline  | Data pipeline for initial load; can be combined with incremental sync.                        | \u274c            | \u2705             | \u274c                      |\n| **Attach MDF / Azure Migrate / Data Box**   | Offline  | File-based or VM lift-and-shift methods.                                                      | \u274c            | \u274c             | \u2705                      |\n\n",
    "category": "Azure SQL Database",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "6 min read",
    "file": "Posts_Curated/azure-sql-database.md"
  },
  {
    "id": 2,
    "title": "Bash Scripting: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to bash scripting, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"Bash Scripting: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"Bash Scripting\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to bash scripting, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# \ud83d\udc1a Bash Tricks: A Guide to Effective Bash Programming\n\n## Table of Contents\n- [TTY (Teletypewriter)](#tty-teletypewriter)\n    - [Modern Meaning](#modern-meaning-linux)\n    - [Identifying Your Terminal](#identifying-your-terminal)\n    - [The Difference Between TTY and TERM](#the-difference-between-tty-and-term)\n    - [Mental Model](#mental-model)\n- [Whiptail: GUI Dialogs for Scripts](#whiptail-gui-dialogs-for-scripts)\n    - [Example Usage](#example-usage)\n\n---\n\n## TTY (Teletypewriter)\n\nA **TTY** is any text terminal interface that:\n*   \u2328\ufe0f Accepts keyboard input\n*   \ud83d\udda5\ufe0f Displays output on a screen\n*   \ud83d\udd04 Provides a way to interact with a program\n\n### Modern Meaning (Linux)\n\n| Type | Name | Example |\n| :--- | :--- | :--- |\n| **Real TTY** | Linux Console | `/dev/tty1` (Access via `Ctrl+Alt+F1`) |\n| **Pseudo TTY** | Terminal Emulator | `gnome-terminal`, `iTerm`, `Alacritty` |\n| **Pseudo TTY** | Remote Session | SSH session |\n| **Not a TTY** | Non-interactive | Background scripts, Pipes `|`, Files, Logs, CI/CD output |\n\n### Identifying Your Terminal\n\n*   `/dev/tty`: The current terminal attached to the process.\n*   `pts`: Pseudo Terminal Slave.\n\nYou can check your current TTY with the `tty` command:\n\n```bash\n\u279c  BASH_SCRIPT tty\n/dev/pts/8\n```\n\n### The Difference Between TTY and TERM\n\nIt is crucial to distinguish between the device and its capabilities:\n\n*   **TTY** (`/dev/tty1`): The actual terminal **device** (connection). Used to send output.\n*   **TERM** (`xterm-256color`): The terminal **type** description. It defines the capabilities (colors, cursor movement). Used to know which control codes to send.\n\n> The `tput` command uses the **TTY** to send data and checks the **TERM** to know *what* data to send.\n\n### Mental Model\n\n> \ud83e\udde0 \"Think of a TTY as a very smart typewriter with a cursor and memory, but no concept of permanence.\"\n\n---\n\n## Whiptail: GUI Dialogs for Scripts\n\n**Whiptail** is a text-based user interface (TUI) library for Bash scripts. It enables you to create user-friendly dialog boxes (like message boxes, input boxes, and menus) directly in the terminal. It is often installed by default on many Linux distributions (e.g., Ubuntu, Debian).\n\n### Example Usage\n\nHere is a simple example of a message box:\n\n```bash\n#!/bin/bash\nset -e\n\n# Display a message box with \"Ok\" button\nwhiptail --msgbox \"Hello, World! \ud83c\udf0d\" 8 40\n```\n\n*   **8**: Height of the box\n*   **40**: Width of the box\n\nWhen you run this script, it will pop up a TUI dialog box in your terminal.\n",
    "category": "Bash Scripting",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "3 min read",
    "file": "Posts_Curated/bash-scripting.md"
  },
  {
    "id": 3,
    "title": "OpenStack Management: Complete Setup, Infrastructure as Code, and Essential Commands",
    "excerpt": "Master OpenStack management from authentication to infrastructure provisioning. Learn how to use Terraform for Infrastructure as Code, configure networking, manage VMs, and essential OpenStack CLI commands for production environments.",
    "content": "---\ntitle: \"OpenStack Management: Complete Setup, Infrastructure as Code, and Essential Commands\"\ndate: \"2025-11-16\"\ncategory: \"Cloud Infrastructure - OpenStack\"\ntags: [\"OpenStack\", \"Infrastructure as Code\", \"Terraform\", \"Cloud Management\", \"DevOps\", \"Networking\"]\nexcerpt: \"Master OpenStack management from authentication to infrastructure provisioning. Learn how to use Terraform for Infrastructure as Code, configure networking, manage VMs, and essential OpenStack CLI commands for production environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# OpenStack Management: Complete Setup, Infrastructure as Code, and Essential Commands\n\nOpenStack is a powerful open-source cloud computing platform that provides Infrastructure as a Service (IaaS). In this comprehensive guide, I'll share everything I've learned managing OpenStack environments\u2014from initial setup and authentication to deploying production infrastructure using Terraform, and the essential commands needed to operate it effectively.\n\n## Introduction: Why OpenStack?\n\nOpenStack has become the backbone of enterprise private cloud deployments worldwide. Whether you're managing a small proof-of-concept or a massive multi-region deployment, OpenStack provides the flexibility and control that proprietary cloud solutions often lack.\n\n**Key advantages I've experienced**:\n- Complete control over your infrastructure\n- No vendor lock-in compared to AWS or Azure\n- Ability to run on-premises or hybrid deployments\n- Cost-effective scaling for large workloads\n\n## Part 1: OpenStack Authentication & RC Files\n\n### Understanding OpenStack RC Files\n\nThe OpenStack RC file (Resource Configuration) is your gateway to authenticating with your cloud. It contains credentials and endpoint information needed for the OpenStack CLI.\n\n```bash\n#!/usr/bin/env bash\n\n# === OPENSTACK RC FILE ===\nexport OS_AUTH_URL=http://<your-openstack-ip>/identity\nexport OS_IDENTITY_API_VERSION=3\nexport OS_INTERFACE=public\n\nexport OS_PROJECT_NAME=\"admin\"\nexport OS_PROJECT_ID=<your-project-id>\nexport OS_PROJECT_DOMAIN_NAME=\"Default\"\nexport OS_USER_DOMAIN_NAME=\"Default\"\n\nexport OS_USERNAME=\"admin\"\nexport OS_PASSWORD=\"<your-password>\"\n\nexport OS_REGION_NAME=\"RegionOne\"\n\n# Optional: disable SSL warnings (if self-signed cert)\n# export OS_INSECURE=true\n```\n\n### Sourcing the RC File\n\nBefore running any OpenStack commands, source your RC file:\n\n```bash\nsource ./admin-openrc.sh\n```\n\nOnce sourced, your environment variables are set and you can interact with OpenStack:\n\n```bash\n# Verify authentication\nopenstack token issue\n\n# See current project\nopenstack project show admin\n```\n\n## Part 2: Infrastructure as Code with Terraform\n\n### Why Terraform for OpenStack?\n\nTerraform provides Infrastructure as Code capabilities for OpenStack, allowing you to:\n- Version control your infrastructure\n- Reproduce environments consistently\n- Collaborate on infrastructure changes\n- Implement CI/CD for infrastructure\n\n### Project Repository\n\nAll Terraform configurations are available in my repository:\n\ud83d\udcc1 **Repo**: [anishkarki/openstack-terraform](https://github.com/anishkarki/openstack-terraform)\n\n### Core Terraform Files\n\n#### 1. Provider Configuration (`provider.tf`)\n\n```terraform\nterraform {\n  required_providers {\n    openstack = {\n      source = \"terraform-provider-openstack/openstack\"\n    }\n  }\n}\n\nprovider \"openstack\" {\n  auth_url    = var.auth_url\n  project_name = var.project_name\n  username    = var.username\n  password    = var.password\n  region      = var.region\n}\n```\n\n#### 2. Variables Configuration (`variables.tf`)\n\n```terraform\nvariable \"config_file\" {\n  default = \"config.yaml\"\n}\n\nvariable \"private_key_path\" {\n  default = \"~/.ssh/id_rsa_terra\"\n}\n```\n\n#### 3. Configuration File (`config.yaml`)\n\n```yaml\nvpc:\n  name: \"prod-vpc\"\n  cidr: \"10.200.0.0/24\"\n  gateway_ip: \"10.200.0.1\"\n  dns_nameservers: [\"8.8.8.8\", \"8.8.4.4\"]\n\nnsg:\n  name: \"prod-nsg\"\n  description: \"Production Network Security Group\"\n  allow_ssh_from: \"0.0.0.0/0\"\n  allow_http: true\n  allow_https: true\n  allow_icmp: true\n\ncluster:\n  keypair_name: \"terra-key\"\n  external_network: \"external\"\n  image: \"Debian 12\"\n  flavor: \"m1.small\"\n\nvms:\n  - name: \"app-1\"\n    ip: \"10.200.0.11\"\n  - name: \"app-2\"\n    ip: \"10.200.0.12\"\n  - name: \"db-1\"\n    ip: \"10.200.0.13\"\n```\n\n#### 4. Main Infrastructure (`main.tf`)\n\nThe main Terraform file orchestrates the entire infrastructure:\n\n**VPC & Networking**:\n```terraform\nresource \"openstack_networking_network_v2\" \"vpc_net\" {\n  name           = local.vpc.name\n  admin_state_up = true\n}\n\nresource \"openstack_networking_subnet_v2\" \"vpc_subnet\" {\n  name            = \"${local.vpc.name}-subnet\"\n  network_id      = openstack_networking_network_v2.vpc_net.id\n  cidr            = local.vpc.cidr\n  gateway_ip      = local.vpc.gateway_ip\n  dns_nameservers = local.vpc.dns_nameservers\n}\n```\n\n**Router & External Gateway**:\n```terraform\nresource \"openstack_networking_router_v2\" \"router\" {\n  name                = \"${local.vpc.name}-router\"\n  admin_state_up      = true\n  external_network_id = data.openstack_networking_network_v2.ext_net.id\n}\n\nresource \"openstack_networking_router_interface_v2\" \"router_int\" {\n  router_id = openstack_networking_router_v2.router.id\n  subnet_id = openstack_networking_subnet_v2.vpc_subnet.id\n}\n```\n\n**Security Group**:\n```terraform\nresource \"openstack_networking_secgroup_v2\" \"nsg\" {\n  name        = local.nsg.name\n  description = local.nsg.description\n}\n\n# SSH\nresource \"openstack_networking_secgroup_rule_v2\" \"ssh\" {\n  direction         = \"ingress\"\n  ethertype         = \"IPv4\"\n  protocol          = \"tcp\"\n  port_range_min    = 22\n  port_range_max    = 22\n  remote_ip_prefix  = local.nsg.allow_ssh_from\n  security_group_id = openstack_networking_secgroup_v2.nsg.id\n}\n\n# HTTP\nresource \"openstack_networking_secgroup_rule_v2\" \"http\" {\n  count             = local.nsg.allow_http ? 1 : 0\n  direction         = \"ingress\"\n  ethertype         = \"IPv4\"\n  protocol          = \"tcp\"\n  port_range_min    = 80\n  port_range_max    = 80\n  remote_ip_prefix  = \"0.0.0.0/0\"\n  security_group_id = openstack_networking_secgroup_v2.nsg.id\n}\n```\n\n**VM Deployment**:\n```terraform\nresource \"openstack_compute_instance_v2\" \"vm\" {\n  for_each = { for vm in local.vms : vm.name => vm }\n\n  name      = each.value.name\n  image_id  = data.openstack_images_image_v2.img.id\n  flavor_id = data.openstack_compute_flavor_v2.flavor.id\n  key_pair  = openstack_compute_keypair_v2.key.name\n\n  network {\n    port = openstack_networking_port_v2.vm_port[each.key].id\n  }\n}\n```\n\n**Floating IPs for External Access**:\n```terraform\nresource \"openstack_networking_floatingip_v2\" \"fip\" {\n  for_each = openstack_networking_port_v2.vm_port\n  pool     = data.openstack_networking_network_v2.ext_net.name\n}\n\nresource \"openstack_networking_floatingip_associate_v2\" \"fip_assoc\" {\n  for_each    = openstack_networking_port_v2.vm_port\n  floating_ip = openstack_networking_floatingip_v2.fip[each.key].address\n  port_id     = each.value.id\n}\n```\n\n### Deploying with Terraform\n\n```bash\n# Initialize Terraform\nterraform init\n\n# Preview changes\nterraform plan\n\n# Apply configuration\nterraform apply\n\n# Destroy when done (use with caution!)\nterraform destroy\n```\n\n## Part 3: Essential OpenStack Management Commands\n\n### 1. Server/Instance Management\n\n#### List All Servers\n```bash\nopenstack server list\nopenstack server list --all-projects  # as admin\n```\n\n#### Create a Server\n```bash\nopenstack server create \\\n  --image <image-id> \\\n  --flavor m1.small \\\n  --key-name <keypair-name> \\\n  --security-group <security-group> \\\n  --nic net-id=<network-id> \\\n  my-server\n```\n\n#### Show Server Details\n```bash\nopenstack server show <server-id>\n```\n\n#### Start/Stop/Reboot Server\n```bash\nopenstack server start <server-id>\nopenstack server stop <server-id>\nopenstack server reboot <server-id>\nopenstack server reboot --hard <server-id>  # Force reboot\n```\n\n#### Delete a Server\n```bash\nopenstack server delete <server-id>\n```\n\n#### Get Server Logs\n```bash\nopenstack console log show <server-id>\nopenstack console url show <server-id>  # VNC console URL\n```\n\n### 2. Floating IP Management\n\n#### List Floating IPs\n```bash\nopenstack floating ip list\n```\n\n#### Create a Floating IP\n```bash\nopenstack floating ip create <external-network>\n```\n\n#### Assign Floating IP to Server\n```bash\nopenstack server add floating ip <server-id> <floating-ip>\n```\n\n#### Remove Floating IP\n```bash\nopenstack server remove floating ip <server-id> <floating-ip>\nopenstack floating ip delete <floating-ip>\n```\n\n### 3. Network & Subnet Management\n\n#### List Networks\n```bash\nopenstack network list\n```\n\n#### Create Network\n```bash\nopenstack network create \\\n  --project <project-id> \\\n  my-network\n```\n\n#### Create Subnet\n```bash\nopenstack subnet create \\\n  --network <network-id> \\\n  --subnet-range 10.0.0.0/24 \\\n  my-subnet\n```\n\n#### Show Network Details\n```bash\nopenstack network show <network-id>\nopenstack subnet show <subnet-id>\n```\n\n### 4. Security Group Management\n\n#### List Security Groups\n```bash\nopenstack security group list\n```\n\n#### Create Security Group\n```bash\nopenstack security group create \\\n  --description \"My Security Group\" \\\n  my-sg\n```\n\n#### Add Security Group Rule\n```bash\n# SSH (port 22)\nopenstack security group rule create \\\n  --protocol tcp \\\n  --dst-port 22:22 \\\n  my-sg\n\n# HTTP (port 80)\nopenstack security group rule create \\\n  --protocol tcp \\\n  --dst-port 80:80 \\\n  my-sg\n\n# Custom port range\nopenstack security group rule create \\\n  --protocol tcp \\\n  --dst-port 8000:8999 \\\n  my-sg\n\n# ICMP (ping)\nopenstack security group rule create \\\n  --protocol icmp \\\n  my-sg\n```\n\n#### Remove Security Group Rule\n```bash\nopenstack security group rule delete <rule-id>\n```\n\n### 5. Image Management\n\n#### List Available Images\n```bash\nopenstack image list\n```\n\n#### Upload Custom Image\n```bash\nopenstack image create \\\n  --file <image-file>.qcow2 \\\n  --disk-format qcow2 \\\n  --container-format bare \\\n  my-custom-image\n```\n\n#### Show Image Details\n```bash\nopenstack image show <image-id>\n```\n\n### 6. Flavor Management\n\n#### List Available Flavors\n```bash\nopenstack flavor list\n```\n\n#### Create Custom Flavor\n```bash\nopenstack flavor create \\\n  --ram 4096 \\\n  --disk 20 \\\n  --vcpus 2 \\\n  --public \\\n  m1.custom\n```\n\n### 7. SSH Key Management\n\n#### List Key Pairs\n```bash\nopenstack keypair list\n```\n\n#### Create Key Pair\n```bash\nopenstack keypair create my-key > my-key.pem\nchmod 600 my-key.pem\n```\n\n#### Delete Key Pair\n```bash\nopenstack keypair delete my-key\n```\n\n### 8. Volume Management\n\n#### List Volumes\n```bash\nopenstack volume list\n```\n\n#### Create Volume\n```bash\nopenstack volume create \\\n  --size 10 \\\n  my-volume\n```\n\n#### Attach Volume to Server\n```bash\nopenstack server add volume <server-id> <volume-id>\n```\n\n#### Detach Volume\n```bash\nopenstack server remove volume <server-id> <volume-id>\n```\n\n#### Delete Volume\n```bash\nopenstack volume delete <volume-id>\n```\n\n### 9. Router Management\n\n#### List Routers\n```bash\nopenstack router list\n```\n\n#### Create Router\n```bash\nopenstack router create my-router\n```\n\n#### Add Subnet to Router\n```bash\nopenstack router add subnet <router-id> <subnet-id>\n```\n\n#### Set External Gateway\n```bash\nopenstack router set \\\n  --external-gateway <external-network> \\\n  my-router\n```\n\n### 10. Project & User Management (Admin)\n\n#### List Projects\n```bash\nopenstack project list\n```\n\n#### Create Project\n```bash\nopenstack project create \\\n  --domain default \\\n  my-project\n```\n\n#### List Users\n```bash\nopenstack user list\n```\n\n#### Create User\n```bash\nopenstack user create \\\n  --domain default \\\n  --password <password> \\\n  my-user\n```\n\n#### Assign Role to User\n```bash\nopenstack role add \\\n  --project <project-id> \\\n  --user <user-id> \\\n  <role-name>\n```\n\n### 11. Quota Management (Admin)\n\n#### Show Project Quotas\n```bash\nopenstack quota show <project-id>\n```\n\n#### Set Quotas\n```bash\nopenstack quota set \\\n  --instances 10 \\\n  --cores 20 \\\n  --ram 10240 \\\n  --volumes 5 \\\n  <project-id>\n```\n\n### 12. Monitoring & Diagnostics\n\n#### Check Service Status\n```bash\nopenstack service list\nopenstack endpoint list\n```\n\n#### Server Diagnostics\n```bash\n# Get CPU, disk, memory stats\nopenstack server lock <server-id>    # Lock server\nopenstack server unlock <server-id>  # Unlock server\nopenstack server pause <server-id>\nopenstack server unpause <server-id>\n```\n\n#### Check Hypervisor Status\n```bash\nopenstack hypervisor list\nopenstack hypervisor show <hypervisor-name>\n```\n\n## Practical Workflow: Deploying a Multi-Tier Application\n\nHere's how I typically deploy a production environment:\n\n### Step 1: Source Credentials\n```bash\nsource ./admin-openrc.sh\n```\n\n### Step 2: Prepare Infrastructure with Terraform\n```bash\ncd openstack-terraform\nterraform init\nterraform plan\nterraform apply -auto-approve\n```\n\n### Step 3: Verify Deployment\n```bash\nopenstack server list\nopenstack floating ip list\n```\n\n### Step 4: SSH into Instances\n```bash\nssh -i ~/.ssh/id_rsa_terra debian@<floating-ip>\n```\n\n### Step 5: Configure Applications\n```bash\n# From the VM\ncurl http://169.254.169.254/latest/meta-data/  # Instance metadata\n```\n\n### Step 6: Monitor and Maintain\n```bash\n# Regular health checks\nopenstack server list\nopenstack volume list\nopenstack floating ip list\n\n# Check resource quotas\nopenstack quota show <project-id>\n\n# Monitor server resources\nopenstack server show <server-id> | grep status\n```\n\n## Best Practices I've Learned\n\n### 1. Always Use Security Groups\nDon't rely on network segmentation alone\u2014use security groups for defense-in-depth.\n\n### 2. Implement Proper Tagging\n```bash\nopenstack server set --property environment=production <server-id>\nopenstack server set --property owner=platform-team <server-id>\n```\n\n### 3. Use Terraform for Everything\nVersion control and automation reduce human error significantly.\n\n### 4. Regular Backups\n```bash\n# Create volume snapshot\nopenstack volume snapshot create \\\n  --volume <volume-id> \\\n  my-snapshot\n```\n\n### 5. Monitor Quotas\nAlways check quotas before deploying:\n```bash\nopenstack quota show <project-id>\nopenstack limits show\n```\n\n### 6. Use Floating IPs Wisely\nFloating IPs are a precious resource. Only assign them when needed for external access.\n\n### 7. Implement Proper Naming Conventions\nUse clear, descriptive names:\n```\nFormat: <environment>-<component>-<version>\nExample: prod-api-v1, staging-db-v2\n```\n\n## Troubleshooting Common Issues\n\n### Server Stuck in Build State\n```bash\nopenstack server reset state <server-id>\nopenstack server delete <server-id>  # Force delete if needed\n```\n\n### Cannot Connect via SSH\n```bash\n# Check security group\nopenstack security group show <sg-id>\n\n# Check floating IP assignment\nopenstack server show <server-id> | grep -E \"floating|addresses\"\n\n# Verify network connectivity\nopenstack network show <network-id>\n```\n\n### Floating IP Already in Use\n```bash\nopenstack floating ip unset <floating-ip>\nopenstack floating ip delete <floating-ip>\nopenstack floating ip create <external-network>\n```\n\n## Advanced Topics\n\n### Automation with Heat (CloudFormation for OpenStack)\n\nHeat is OpenStack's orchestration engine:\n\n```bash\n# List stacks\nopenstack stack list\n\n# Create stack from template\nopenstack stack create \\\n  -t my-template.yaml \\\n  my-stack\n\n# Delete stack\nopenstack stack delete my-stack\n```\n\n### Volume Snapshots and Backups\n\n```bash\n# Create snapshot\nopenstack volume snapshot create \\\n  --volume <volume-id> \\\n  my-snapshot\n\n# Create backup\nopenstack volume backup create <volume-id>\n\n# List backups\nopenstack volume backup list\n```\n\n## Conclusion: Mastering OpenStack Management\n\nOpenStack provides tremendous flexibility and control for managing your infrastructure. Whether you're using Terraform for Infrastructure as Code or managing resources through the CLI, the key is to:\n\n1. **Automate Everything** - Use Terraform for reproducibility\n2. **Document Your Setup** - Keep RC files and configurations versioned\n3. **Monitor Continuously** - Regular checks prevent surprises\n4. **Use Security Groups** - Implement proper network segmentation\n5. **Plan for Scale** - Design with growth in mind\n\n## Resources\n\n- \ud83d\udcc1 **Terraform Repository**: [anishkarki/openstack-terraform](https://github.com/anishkarki/openstack-terraform)\n- \ud83d\udcda **OpenStack Documentation**: [docs.openstack.org](https://docs.openstack.org)\n- \ud83d\udd27 **Terraform Provider**: [registry.terraform.io/providers/terraform-provider-openstack/openstack](https://registry.terraform.io/providers/terraform-provider-openstack/openstack)\n\n---\n\n*Managing complex cloud infrastructure doesn't have to be overwhelming. Start with Terraform, master the essential CLI commands, and build from there. Happy infrastructure coding!*\n\n*Have questions about OpenStack management or want to discuss infrastructure automation? Connect with me on [LinkedIn](https://www.linkedin.com/in/anish-karki-dba/) or [send me an email](mailto:anish.karki1.618@outlook.com).*\n\n",
    "category": "Cloud Infrastructure - OpenStack",
    "tags": [
      "OpenStack",
      "Infrastructure as Code",
      "Terraform",
      "Cloud Management",
      "DevOps",
      "Networking"
    ],
    "date": "2025-11-16",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "10 min read",
    "file": "Posts_Curated/cloud-infrastructure-openstack.md"
  },
  {
    "id": 4,
    "title": "\ud83e\udde0 Oracle VCN Setup: MLResearch (Full Annotated Version)",
    "excerpt": "This markdown outlines the Virtual Cloud Network (VCN) named MLResearch in Oracle Cloud, including definitions, functions, and rationale for key configurations such as route tables, security lists, ga...",
    "content": "---\ntitle: \"\ud83e\udde0 Oracle VCN Setup: MLResearch (Full Annotated Version)\"\ndate: \"2026-01-12\"\ncategory: \"Cloud Networking\"\ntags: [\"Cloud Networking\"]\nexcerpt: \"This markdown outlines the Virtual Cloud Network (VCN) named MLResearch in Oracle Cloud, including definitions, functions, and rationale for key configurations such as route tables, security lists, ga...\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# \ud83e\udde0 Oracle VCN Setup: MLResearch (Full Annotated Version)\n\nThis markdown outlines the **Virtual Cloud Network (VCN)** named `MLResearch` in Oracle Cloud, including **definitions**, **functions**, and **rationale** for key configurations such as route tables, security lists, gateways, and CIDR blocks.\n\n---\n\n## \ud83c\udf10 What is a VCN?\n\nA **Virtual Cloud Network (VCN)** is a logically isolated network in Oracle Cloud Infrastructure (OCI), similar to a traditional data center network, including subnets, gateways, route tables, and firewalls (security lists).\n\n---\n\n## \ud83d\udd27 VCN Configuration\n\n| Key         | Value                            |\n|-------------|----------------------------------|\n| Name        | MLResearch                       |\n| Compartment | MachineLearningAI                |\n| CIDR Block  | 10.0.0.0/16                      |\n| DNS Domain  | MLResearch.oraclevcn.com         |\n\n> \ud83d\udd39 `10.0.0.0/16` gives us 65,536 IPs, to be divided into subnets.\n\n---\n\n## \ud83d\udccd Subnet Design\n\nSubnets divide the VCN into **public** and **private** zones for secure resource separation.\n\n### \ud83c\udf10 Public Subnet\n\n| Detail        | Value                    |\n|---------------|--------------------------|\n| Name          | public subnet-MLResearch |\n| CIDR          | 10.0.0.0/24              |\n| Security List | default                  |\n| Route Table   | default                  |\n\n- **Purpose:** Hosts internet-facing resources (e.g., Bastion or Web App)\n- **Access:** Has **direct** internet access via **Internet Gateway**\n\n### \ud83d\udd12 Private Subnet\n\n| Detail        | Value                           |\n|---------------|---------------------------------|\n| Name          | private subnet-MLResearch       |\n| CIDR          | 10.0.1.0/24                     |\n| Security List | private subnet security list    |\n| Route Table   | private subnet route table      |\n\n- **Purpose:** Hosts secure, internal resources (e.g., databases, APIs)\n- **Access:** Only outbound internet via **NAT Gateway**, and OCI services via **Service Gateway**\n\n---\n\n## \ud83d\udeaa Gateways\n\n| Name                     | Type             | Function                                                                 |\n|--------------------------|------------------|--------------------------------------------------------------------------|\n| Internet gateway-MLResearch | Internet Gateway | Allows public subnet VMs to send/receive internet traffic                |\n| NAT gateway-MLResearch      | NAT Gateway      | Allows private subnet VMs to make **outbound** internet requests only    |\n| Service gateway-MLResearch  | Service Gateway  | Allows private subnet to access OCI services (like Object Storage) **privately** |\n\n---\n\n## \ud83d\udee3\ufe0f Route Tables Explained\n\n### \ud83d\udd39 Why use `0.0.0.0/0`?\n\n- **Definition:** `0.0.0.0/0` matches **any IP address** \u2014 it's the **default route** for external/unmatched traffic.\n- **Purpose:** Used to direct **internet-bound** traffic or OCI API traffic via gateways.\n\n### \ud83d\udcd8 default route table for MLResearch (used by public subnet)\n\n| Destination    | Target           |\n|----------------|------------------|\n| `0.0.0.0/0`    | Internet Gateway |\n\n> \u2705 Sends *all* outbound traffic to the internet.\n\n### \ud83d\udcd8 route table for private subnet-MLResearch\n\n| Destination                | Target          |\n|----------------------------|------------------|\n| `0.0.0.0/0`                | NAT Gateway     |\n| All SYD Oracle Services    | Service Gateway |\n\n> \u2705 Keeps traffic private & secure while still allowing outbound and service access.\n\n---\n\n## \ud83d\udd10 Security Lists Explained\n\nSecurity lists control **inbound (ingress)** and **outbound (egress)** traffic to VMs.\n\n### \ud83d\udd39 Why `0.0.0.0/0` in Ingress?\n\n- **Means:** Allow access from *anywhere*.\n- **Use Case:** Public-facing SSH or HTTP/HTTPS traffic.\n\n> \u26a0\ufe0f Use sparingly \u2014 it's **open to the world**. Limit using IP whitelisting if possible.\n\n### \ud83d\udd39 Why `0.0.0.0/0` in Egress?\n\n- **Means:** Allow outbound traffic to *anywhere*.\n- **Use Case:** Allows updates, package installs, API calls.\n\n> \u2705 Common and often necessary unless you\u2019re enforcing strict egress controls.\n\n### \ud83d\udee1\ufe0f default security list for MLResearch (public subnet)\n\n| Type     | Source/Destination | Protocol    | Ports/Details                          |\n|----------|--------------------|-------------|----------------------------------------|\n| Ingress  | `0.0.0.0/0`        | TCP         | Port 22 (SSH)                          |\n| Ingress  | `0.0.0.0/0`        | ICMP        | Type 3 (Destination unreachable)       |\n| Egress   | `0.0.0.0/0`        | ALL         | All traffic                            |\n\n### \ud83d\udee1\ufe0f security list for private subnet-MLResearch\n\n| Type     | Source/Destination | Protocol    | Ports/Details                            |\n|----------|--------------------|-------------|------------------------------------------|\n| Ingress  | `10.0.0.0/16`      | TCP         | Port 22 (SSH from within VCN)            |\n| Ingress  | `0.0.0.0/0`        | ICMP        | Type 3 (unreachable), Type 4 (fragment)  |\n| Egress   | `0.0.0.0/0`        | ALL         | All traffic                              |\n\n---\n\n## \ud83e\udde0 Summary of Core Concepts\n\n| Concept              | Meaning                                                     | Why It's Used                                                                 |\n|----------------------|-------------------------------------------------------------|--------------------------------------------------------------------------------|\n| `0.0.0.0/0` in Route | Default route \u2014 covers all IPs                              | Needed for internet or service access                                          |\n| `0.0.0.0/0` in Ingress| Allow access from anywhere                                 | Used for SSH/public ports on bastion                                           |\n| `0.0.0.0/0` in Egress | Allow all outbound traffic                                 | Required for app updates, external APIs                                        |\n| Internet Gateway     | Public IP internet access                                  | Connects public subnet VMs to the internet                                    |\n| NAT Gateway          | Secure outbound access from private subnet                | Enables internet updates without public IP                                    |\n| Service Gateway      | Private access to Oracle services                          | Keeps traffic private and avoids internet                                     |\n\n---\n\n## \ud83d\udcc8 Network Architecture Diagram\n\nRefer to the attached network diagram that visually explains how the VCN, subnets, gateways, and OCI services connect and interact.\n\n<img src=\"VCN.png\" alt=\"Description\" width=\"500\" height=\"300\">\n\n",
    "category": "Cloud Networking",
    "tags": [
      "Cloud Networking"
    ],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "4 min read",
    "file": "Posts_Curated/cloud-networking.md"
  },
  {
    "id": 5,
    "title": "OCI Data Science: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to oci data science, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"OCI Data Science: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"OCI Data Science\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to oci data science, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# Types of storage in OCI Data Science\n| Storage Type | Description |\n|--------------|-------------|\n| Object Storage | Used for storing large datasets, models, and other files. It is highly scalable and durable. |\n| Block Volume | Provides high-performance block storage for compute instances. Useful for applications requiring low-latency access to data. |\n| File Storage | Offers a managed file system that can be mounted to multiple compute instances, suitable for shared access to files. |\n| Database | Managed databases for structured data storage, such as Oracle Database or MySQL. Useful for applications requiring relational data management. |\n\n## Types of Block storage\n| Block Storage Type | Description |\n|--------------------|-------------|\n| iSCSI Block Volumes | Provides block storage over the iSCSI protocol, suitable for a wide range of applications. |\n| Fibre Channel Volumes | High-performance block storage accessed over Fibre Channel networks, ideal for enterprise applications. |\n| Local NVMe Disks | High-speed local storage attached directly to compute instances, offering low-latency access to data. |\n\n## Modern features of block storage in OCI\n| Feature | Description |\n|---------|-------------|\n| Elastic Volumes | Allows for dynamic resizing of block volumes without downtime, providing flexibility for changing workloads. |\n| Data Encryption | Supports encryption of data at rest and in transit, ensuring data security and compliance. |\n| Snapshot and Cloning | Enables point-in-time snapshots and cloning of block volumes for backup and recovery purposes. |\n\n    * ONNX: Open source format for representing machine learning models, allowing interoperability between different frameworks.\n\n* work with Oracle with ADS connector, sqlalchemy and ipython-sql\n* Data-flow is OCI spark for distributed data processing.\n* sparksql-magic for running Spark SQL queries in Jupyter notebooks.\n\n#### slug: oci-data-science-notebook-session\n* Browse\n* search\n* install\n* clone\n* modigy\n* publish: initialisize conda bucket, publish --slug SLUG\n* delete\n* create from YAML: conda create --from-yaml <yaml-file>\n---\n* A conda slug is a versioned collection of conda packages that can be used to create a reproducible environment for running Jupyter notebooks in Oracle Cloud Infrastructure (OCI) Data Science. It allows users to share and deploy their data science environments easily. \\\nHow to create a conda slug:\n  1. Create a conda environment with the required packages.\n  2. Use the `conda slug create` command to package the environment into a slug.\n  3. Publish the slug to OCI Data Science for use in notebook sessions.\n\n### OCI Valult\n#### Introuduction\n* ceneralized service for managing secrets, such as API keys, passwords, and certificates.\n* Provides secure storage and access control for sensitive information.\n* AES, RSA, ECDSA for encryption and decryption.\n---\n1. Virtual private vaults for isolating partition in HSM. \n2. Vault in a shared partition for shared access to secrets. can't backup.\n\n* keys: logical representation of cryptographic keys used for encryption and decryption.zEncrypt or in digital signing operations.\n    * master encryption keys (MEKs): used to encrypt and decrypt data.\n    * Data encryption keys (DEKs): used to encrypt and decrypt specific data. Envalope encryption is used to protect DEKs with MEKs.\n    * Rotating key: periodically changing the keys used for encryption to enhance security. (older is saved behind the scene.)\n* secrets: sensitive information such as API keys, passwords, and certificates.\n    * create using the console, SDK, CLI or API and rotate with version.\n\n&rarr; OCI-Valuts: Keys, FIPS 140-2 level 3 certificate, secrets.\n\n## Oracle manged keys:\na. Master key + oracle vault \\\nb. Customer key + customer vault\n\n* Data at rest is always encrypted.\n* buckets are not allowed to use oracle manage key, customer needs to use their own key.\n---\n* **ADBSecretkeeper**: for autonomous database\n* **BDSSecreetKeeper**: big data service\n* **MySQLDBsecretkeeper**: mysql\n* **Authtokensecretkeeper**:  auth token or access token string. streming, github.\n---\n\n\n\n\n",
    "category": "OCI Data Science",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "4 min read",
    "file": "Posts_Curated/oci-data-science.md"
  },
  {
    "id": 6,
    "title": "PostgreSQL Administration: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to postgresql administration, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"PostgreSQL Administration: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"PostgreSQL Administration\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to postgresql administration, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# Database Architectures\n## SQL SERVER\n1. monolithic, multi-Layered Architecture within single sqlserver.exe\n    * Client Application\n    * &darr; (TDS tabular data stream orver TCP/IP or named pipes)\n    * Protocol Layer (SMI) (handles network protocols)\n    * &darr;\n    * Relational Engine (Quer Processor)\n        * Commond Parser: validates syntax and semantics\n        * algebrizer: resolves name and objects\n        * Optimizer: Multiple possible exection plans for a query, Estimates their IO, CPU and selects the most effieicit one\n        * query executor: connector of plan and storage engine\n    * &darr;\n    * Storage Engine: \n        * Access Methods: Heap, B-tree index and Columnstore access.\n        * Buffer Manger (Plan cache, data cache): Cache data pages in memoery LRU\n        * Txm manager or log manager: manages write-ahead logging\n        * lock manager: controls concurrency. \n    * &darr;\n    * SQLOS: a user-mode application layer between SQL server and windows os. Handles OS_like services such as thread scheduling, memory management and I/O management specifically tailored for SQL server's needs. \n        * thread scheduling\n        * memory management\n        * I/O management\n* SQL server uses cooperative scheduling via SQLOS - not relying solely on windows thread scheduler. \n\n## Postgresql\nIt is a different beast. It has a sinlge master postmaster running and for each client connection it forks a new process (not thread). \n* Postmaster is reponsible for forking the backend processes that uses shared memeory content. It also handles background processes like ```autovaccum, checkpoint, WAL writer, stats collectors```.\n* Background workers:\n* Client connection: if you have 2 processors and more than 2 conneciton, there will be context switching overhead. Processes share CPU time slices fairly. Use tools like PgBouncer and Pgpool-II keep a pool of backend processes. \n\n### Parts of Server's Storage Engine?\n1. Access methods\n2. Buffer manaer (buffer pool, plan cache) LRU-K algorithms\n3. Tranaction log manager: WAL and LSN\n4. Lock manger: row/pages/db locks\n    * RID, Page, EXTENT, TABLE, DB (S, X, U, Intent)\n5. Recovery manager: REDO/UnDO using write-ahead log during startups.\n```All data modification goes through locks first for durability.```\n#### CHECK\n* RID, Page, EXTENT, TABLE, DB (S, X, U, Intent)\n### Postmaster\n#### Backend process\n1. Handles exactly one client sessions.\n2. Has its own memory context but shares\n    * shared buffer\n    * WAL buffers\n    * lock tables\n    * stats\n3. Access to the data files, WAL files\n\n### Database instance model sql serer vs cluster model postgreSQL\n1. SQL SERVER - Instance model\n    * running copy of sql server services (engine, agent, broker etc)\n    * Isolated security and configuration\n    * listen to different ports.\n2. Postgresql\n    * a single postmaster.\n    * Multiple databases in single cluster\n    * all databases in a cluster shares\n        * postmaster\n        * port\n        * memory and background workers\n        * same global objects (tables, tablespaces)\n        * roles, pg_dba.conf, WAL, Config\n    * ``` Databases are isolated and can't join across them without postgres_fdw or dblink.```\n\n### Comparision of Transaction log and WAL\n1. PostgreSQL:\n    * Wal for durability and enables crash recovery and replication\n    * all changes in WAL segments (16MB each in pg_wal/)\n    * WAL records decribe low-level changes (\"set x of page y to z\") low level record.\n    * ```Checkpoints: they flush dirty buffers to data files -- WAL before data```\n    * Used for crash no UNDO always use MVCC.\n    * Point-in-time recovery. \n    * WAL record is flushed to disk (fsync) before the dirty page itself is written back. That\u2019s why it\u2019s called Write-Ahead Logging \u2192 logs are always persisted first.\n    * WAL is physical not logical like mysql\n    * WAL entry: page 1232, offset 200, new tuple inserted.\n    * lazy writer for dirty page but WAL is immediate to disk (fsync)\n\n* Restart the crashed postgresql\n    * reads the WAL segment files (PG_WAL/) and finds the last checkpoint record (safe known state) using LSN and replay WAL. \n* Checkpoint ( checkpointer + background writer ) triggers Triggered by:\n    *   WAL size growth (max_wal_size).\n    *   Time interval (checkpoint_timeout).\n    *    Manual command: CHECKPOINT;.\n    *   Shutdown (always does a final checkpoint).\n\n2. SQL server Txn log:\n    * uses WAL - all modifications logged before being written to data files.\n    * Log records are the logical operations (e,g Insert ROW r into table t).\n    * supports rollback- contains both REDO and UNDO information.\n    * divied into virtual log files (VLFs) -- internal segements in ```(.ldfs)```\n        * used in crash recovery\n        * replication (txn, always on)\n        * log shipping\n        * CDC, temporal tables.\n    * Log records are logical + physical ( unlike postgresql with only physical )\n        * example (logical desc: insert x into table y)\n        * enough is stored to redo committed txn and undo uncommitted ones.\n\n    * VLFs: there is at least one log file. SQL Server breaks this into smaller chunks called VLFs.  Internal division for easier management. <64 is 4 VLFs, 64 to 1G is 8 VLFs and >1 is 16 VLFs. Circular fashion when last is full comes to first and use VLF that are no longer needed. The crality is after the checkpoint or log backup in FULL recovery model, old VLFs are inactive. That\u2019s why DBAs recommend setting reasonable initial size + growth increments.\n\n* there is a checkpoint process (no lazy writer)\n* Triggered by \n    * Automatic *(based on recovery interval settings)\n    * indirect checkpoint (per-database, smoother writes)\n    * manual checkpoint;\n    * shutdown\n* CYCLE (ARIES model)  \n\n```lazy writer = memory manager, checkpoint = durability & recovery manager.```\n##### NOTE: in case of postgresql there is background writer that works continuously and flush dirty pages gradually to reduce checkpoint spikes. and there is checkpointer, all dirty pages to disk and record to WAL, like after the full backup. everything before this WAL lSN is persistent. now you can truncate the WAL. The same kind of lazy writer occur in sql server as well. The checkpoint worker does flush and end checkpoint log record in txn log and records active txns needed for UNDO phase. After checkpoint the inactive VLFs can be truncated or reused. \n\n### TEMP DB\n* system db -- recreated every time sql server restarts\n* used for:\n    * temporary tables, temporary variables\n    * sorts, hashes, spools (query workspaces)\n    * version store ( for RCSI, online index rebuilds, triggers)\n    * cursors, LOB variables\n    \n\n    \n\n\n\n\n\n\n\n\n\n\n",
    "category": "PostgreSQL Administration",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "6 min read",
    "file": "Posts_Curated/postgresql-administration.md"
  },
  {
    "id": 7,
    "title": "Python Rich Library: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to python rich library, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"Python Rich Library: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"Python Rich Library\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to python rich library, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n#SparkTutorial1\n# SPARK TUTORIAL\n## Python os and path\n<style>\n.impressive-paragraph {\n  font-size: 12px;\n  font-weight: bold;\n  color: #0f172a;\n  background: linear-gradient(to right, #e0f2fe, #bae6fd);\n  padding: 20px;\n  border-left: 6px solid #0284c7;\n  border-radius: 8px;\n  box-shadow: 0 4px 10px rgba(0,0,0,0.1);\n  font-family: 'Segoe UI', sans-serif;\n}\n</style>\n\n```Python\nimport os\nimport path\n\n## Check the existance of the file and directory.\ncurrent_dir = os.getcwd() # get current working directory\nprint(f\"{current_dir}\") \nfull_path = os.path.join(\"/lakehouse/default/Files/chicago_crimes.csv\") #Create the full path to any file. \n# Get the directory name and base name\nprint(f\"For {full_path} dirname:{os.path.dirname(full_path)}, basenaem: {os.path.basename(full_path)}\")\nprint(os.path.split(full_path)) # Can also use the split function\nprint(os.path.splitext(os.path.basename(full_path))) # split the extensions\n## checking the existance\nprint(os.path.exists(full_path)) #both file or directory\nprint(os.path.isfile(full_path), os.path.isdir(full_path)) # both check\nprint(os.path.isabs(full_path)) # check for absolute and relative\nprint(os.path.normpath(full_path)) # just remove any unnecessary thing\n\n## File handeling\nimport shutil\ntarget_path=\"/lakehouse/default/Files/Bronze\"\nif os.path.isdir(target_path):\n    #shutil.move(full_path, target_path)\n    pass\nimport shutil\ntarget_path=\"/lakehouse/default/Files/Bronze\"\nfile_path = os.path.join(target_path, \"chicago_crimes.csv\")\nif os.path.isdir(\"/lakehouse/default/Files/\"):\n    shutil.move(file_path, \"/lakehouse/default/Files/\")\n\nos.rename(full_path, os.path.join(os.path.dirname(full_path),\"chicago_crimes.csv\"))\n# os.remove() --for deleteing file\n# shutil.rmtree(<path>) # remoe the directory and its conetnet.\n# shutil.copy(source, destinationdir or fullpath)\n```\n|Library | function|\n|---|--|\nos | this can manipulate the os and files and path of the underlying os directly (dangerous)\nshutil | special library that can help to manipulate the files. Rename/copy/delete/create\n---\n### Directory structures in lakehouse\n* File API path: This shows that the file are in the default part of the lakehouse. virtual path: /lakehouse/default/Files/chicago_crimes.csv\n* Absolute ABFS Path: \"abfss://workspacedev@onelake.dfs.fabric.microsoft.com/CrimeAnalysis.Lakehouse/Files/chicago_crimes.csv\"\n   * absolute path contains the root fo the lakehouse in azure blob storage.\n* Realtive path: this is the relative path. It doesn't have the root. Files/chicago_crimes.csv. Location inside the lakhouse.\n\n### Entry point for all the spark.\n#### SparkSession: This is the abstraction of HIVE features inclduing ability to write HiveQL, access to HIVE UDFs and ability to read data from Hive tables. \n> * Microsoft has tweaked it into something of their own called Spark SQL that access the fabric lakehouse metdata store data in OneLake (Delta Lake), the file format is Delta and catalog acess is spark.catalog\n> * The ```sparksession``` wraps the **sparkcontext**, which handles the actual engine execution.\n```py\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n```\n* *SparkContext*\n   * manages connection to the spark cluster\n   * tracks the resources\n   * manages job and stage exection\n\nFeatures:\n1. session lifecycle managed with notebook\n2. Stateless between sessions - metadata stored in lakehouse\n3. fully managed pool ( no manual clustering)\n4. session isolation: Each notebook/job has its own session and context\n5. integrated with AAD.\n6. No actual hoddop or YARN ResourceManager. Custom orchestration layer that abstracts spark jobs.\n\n<p class=\"impressive-paragraph\">\n  Apache Spark is a distributed computing engine designed for fast and scalable data processing. It works by breaking large datasets into smaller chunks called partitions, which are processed in parallel across a cluster of machines. At the core is the SparkSession, which acts as the entry point and manages the job lifecycle. When you run a transformation (like map, filter, or groupBy), Spark builds a Directed Acyclic Graph (DAG) that represents the logical execution plan. Once an action (like collect() or write()) is triggered, Spark optimizes the DAG into physical stages and schedules tasks across executors\u2014worker processes that run in parallel. Each task operates on a partition, allowing efficient use of memory and CPU across nodes. Unlike Python\u2019s threading or multiprocessing, Spark runs distributed tasks at cluster scale. In Microsoft Fabric, Spark is fully managed: you don't need to set up clusters or manage resources\u2014Fabric automatically provisions compute, handles partitioning, and executes your Spark code seamlessly in the cloud.\n</p>\n\n* The beauty is it created DAG (logical exection plan) and optimizes them before exection. So the initial part is slow but exection is really fast.\n\n### Reading the FILE\n```py\nfrom pyspark.sql.functions import to_timestamp, col, lit\n```\n\n### Ttpes of dataframe API\n1. dataframes (highlevel API): Spark dataframes are partitioned and sits on the hundreds of computers with processors.\n2. RDD (resilient distributed datasets)\n\n### Handeling the CSV, URL, Database connection to fetch data\n* CSV load: ```df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)```\n* URL: You can't do it. need to use python ```urllib.request.urlretrive()```\n* Database: use jdbc connector to connect onto any realtional database (SQL server, PostgreSQL, MySQL)\n```py\njdbc_url = \"jdbc:sqlserver://<hostname>:<port>;databaseName=<dbname>\"\nconnection_properties = {\n    \"user\": \"your_username\",\n    \"password\": \"your_password\",\n    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\ndf = spark.read.jdbc(url=jdbc_url, table=\"dbo.YourTable\", properties=connection_properties)\n```\n### Loading data with schema\n1. Reading data with known schema\n   * improves performance\n   * prevents wrong data types\n   * avoid nulls caused by parsing errors\n---\n## Common daily syntaxs\n|syntax-spark |Syntax-pandas | functions|\n|---|---|---|\n|```df.select([<rows>]).show(5)```|```df[[<name of columns>]]``` | select the specific columns from the dataframe and limit to 5 rows|\n|```df.withColumnRenamed(ExistingColumnName, NewColumnName)``` | ```df.rename(columns={'Existing':'NewName'})``` | Rename the existing column|\n| rc.withColumn('One',lit(1)) |  |add a column with anem One; with entries all 1s.\n| df.remove(<colname>)| |remove a solumn from dataframe\n\n\n\n\n#### Lit function: It is used to create a column with a constant literal value. Unlike pandas pyspark requires everything to to be a column object. lit() wraps literal values so that can be used like column exp.\n```df_spark.filter(df_spark['age']>lit(30)).show()```\n\n### Using pivot keyword to make it \n```py\n.groupby().pivot(<pivotkey>).sum()\npivot_df = df_spark.groupBy(col('arrest')).count().groupBy().pivot('arrest').sum('count')\n```\n\n## Important Collect()[0]['ratio']\n* Collect(): returns all rows to the driver like a cursor. A list of Row objects, used to pull small results into the python memeory.\n* .show() print content to console\n\n```Python\n# What % of reported crimes that resulted in an arrest\npivot_df = df_spark.groupBy(col('arrest')).count().groupBy().pivot('arrest').sum('count')\nratio = pivot_df.select((col(\"`true`\") / df_spark.count()).alias(\"ratio\")).collect()[0][\"ratio\"]\nprint(f\"True Arrest Ratio: {ratio:.4f}\")\n\n# Straight forward\ntrue_count = df_spark.filter(col('arrest')=='true').count()\ntotal_count = df_spark.count()\nratio = true_count/total_count\nprint(ratio)\n\n# Using agg() with when()\nfrom pyspark.sql.functions import when, count, col\n\nagg_df = df_spark.agg(\n    count(when(col('arrest')=='true',1)).alias(\"true_count\"),\n    count(\"*\").alias('total_count')\n)\n\nratio = agg_df.select((col('true_count')/col('total_count')).alias(\"arrest_ratio\")).collect()[0]\nprint(ratio)\n```\n\n\n\n\n\n\n",
    "category": "Python Rich Library",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "5 min read",
    "file": "Posts_Curated/python-rich-library.md"
  },
  {
    "id": 8,
    "title": "SQL Server Administration: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to sql server administration, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"SQL Server Administration: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"SQL Server Administration\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to sql server administration, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# Oracle Autonomous Database Dedicated Infrastructure \u2013 Key Points\n\n## Deployment Options\n- **Serverless:**  \n  - Fully automated infrastructure and database management (provisioning, tuning, backup, scaling).  \n  - Scales instantly based on workload.  \n- **Dedicated:**  \n  - Private database cloud on dedicated Exadata infrastructure (public cloud or Cloud@Customer).  \n  - Complete isolation from other tenants, customizable policies, software updates, and availability.  \n  - Ideal for consolidating multiple databases and offering DBaaS within an enterprise.  \n\n## Dedicated Infrastructure Features\n- Requires **Exadata Cloud subscription** with dedicated compute, storage, memory, and network.  \n- Supports **Autonomous VM Clusters (AVMC)** and **Autonomous Container Databases (ACD)** for workload isolation.  \n- Network is via **VCN/subnet**, private by default (no public internet access).  \n- Oracle manages **DB, hypervisor, OS, and hardware**; customer manages **data schemas and encryption keys**.  \n- Supports **network services**: VCN peering, IPsec, VPN, FastConnect.  \n\n## Cloud@Customer\n- Brings Exadata and Autonomous Database behind **customer firewall** for strict data sovereignty.  \n- High performance:  \n  - Up to 45\u00d7 higher SQL read IOPS  \n  - 40\u00d7 higher SQL throughput  \n  - 98% lower SQL latency than AWS RDS Outposts  \n- Control plane via **secure WebSocket tunnel** to Oracle Cloud; continuous monitoring and patching.  \n- Exadata DB server connectivity: 10/25/100 Gbps, RDMA over Converged Ethernet internally.  \n- Autonomous Database continues **normal operations even if control plane connectivity is lost**.  \n\n## Security & Backup\n- **Transparent Data Encryption (TDE):** Automatically encrypts/decrypts data for authorized users.  \n- Backup options:  \n  - On-premises  \n  - Zero Data Loss Recovery Appliance  \n  - Locally mounted NFS storage  \n  - Oracle Public Cloud storage  \n\n## Summary\n- Dedicated infrastructure provides **high security, isolation, and control**, while still offering **automated scaling, backups, and Oracle management**.  \n- Cloud@Customer combines **cloud automation and local data control**, ideal for regulatory, performance, or legacy integration requirements.\n\n---\n# Autonomous Database Dedicated \u2013 Workflow & Functionality\n\n## Workflow\n1. **Fleet Administrator:**  \n   - Provisions Exadata infrastructure.  \n   - Partitions system with Autonomous VM Clusters (AVMC) and Autonomous Container Databases (ACD).  \n2. **Developers/DBAs:**  \n   - Provision databases within container databases (self-service model).  \n   - Optionally set up their own container databases.  \n\n## Billing\n- **Infrastructure cost:** Based on Exadata size, number of databases, storage.  \n- **Database compute:** Cost based on active CPUs used by Autonomous Databases.  \n- **Provisioning of AVMC/ACD:** No cost until databases are running.  \n\n## Infrastructure & Shapes\n- Supported Exadata shapes: **X9M, X10M, X11M**.  \n- X10M/X11M offer **elastic shapes** for flexible resource allocation.  \n\n## Getting Started\n1. Request **service limit increase** for Exadata Rack.  \n2. Create **fleet and DBA roles**.  \n3. Set up **private cloud** with OCI compartments.  \n4. Create **network model** and resources (AVMC, ACD).  \n5. Provide **access to end users** for provisioning databases.  \n\n## Features & Capabilities\n- Minimum subscription: **48 hours**, no ongoing cost after termination.  \n- Control over **patch scheduling** and **software versioning**.  \n- Database migration via **export/import, Data Pump, or GoldenGate**.  \n- **Auto-scaling** and **cloning** available.  \n- **Dedicated vs Serverless:**  \n  - Private, single-tenant infrastructure  \n  - Workload separation (containers, VM clusters)  \n  - Separate dev/test/prod environments  \n  - Custom maintenance scheduling  \n  - Multiple DB versions, sharding, in-memory support, synchronous/asynchronous replication, Active Data Guard  \n  - Regular TCP connections (no wallet needed)  \n  - Note: Oracle Machine Learning Notebooks only in serverless  \n\n## Summary\nAutonomous Database Dedicated provides **private, flexible, and highly customizable infrastructure** for enterprise workloads while still supporting **self-service provisioning, scaling, and cloning** for end users.\n\n---\n#  Provisioning Dedicated Resources\n\n**Lesson Focus:** Setting up dedicated infrastructure and provisioning an Autonomous Database on Exadata.\n\n---\n\n## 1. Provisioning Exadata Infrastructure\n1. Sign in to **Oracle Cloud (OCI)** as a **network or fleet administrator**.\n2. Navigate to **Autonomous Database \u2192 Autonomous Exadata Infrastructure \u2192 Create**.\n3. Configure:\n   - **Display name** (meaningful)\n   - **Availability domain**\n   - **Shape**: Quarter Rack (92 OCPUs)\n   - **Subnet selection**\n   - **Maintenance schedule** (default or custom, e.g., quarterly)\n   - **License type**\n4. Infrastructure is provisioned in a few minutes.\n\n---\n\n## 2. Network Setup\n1. Create a **VCN** in the fleet compartment (CIDR `10.0.0.0/16`) for subnets.\n2. Add **security lists** for:\n   - Exadata subnet\n   - Application subnet\n3. Two-tier setup:\n   - Exadata subnet: hosts the database.\n   - Application subnet: hosts VPN, app servers, bastion hosts, etc.\n4. Optional **Internet access**:\n   - Deploy **Internet Gateway**.\n   - Configure **route tables** (`0.0.0.0/0` for public access if needed).\n\n### Subnet Configuration\n| Subnet | CIDR Block | Route Table | Notes |\n|--------|------------|------------|-------|\n| Exadata | 10.0.0.0/24 | Default | Database hosts only |\n| Application | 10.0.1.0/24 | Custom (internet access) | App servers, bastion hosts |\n\n---\n\n## 3. Provisioning Autonomous Container Database (ACD)\n1. Log in as **fleet administrator**.\n2. Navigate to **Autonomous Transaction Processing \u2192 Autonomous Container Database \u2192 Create**.\n3. Configure:\n   - Compartment hosting Exadata\n   - Apply **Release Update (RU)** if needed\n   - **Maintenance schedule** (monthly/quarterly)\n   - **Backup retention** (up to 60 days)\n4. Click **Create Autonomous Container Database**.\n\n---\n\n## 4. Provisioning Autonomous Database on Dedicated Infrastructure\n1. Log in as **database user**.\n2. Navigate to **Autonomous Transaction Processing / Autonomous Data Warehouse \u2192 Create**.\n3. Configure:\n   - Database name\n   - **Dedicated infrastructure**\n   - Select **compartment** and **Autonomous Container Database**\n4. Click **Create Autonomous Database**.\n\n---\n\n**Key Notes:**\n- Two-tier network: Exadata (DB) + Application (compute/servers)\n- Flexible **maintenance schedules** and **backup retention**\n- Dedicated autonomous databases use **container databases (ACD)** for multitenancy\n- Internet access and security lists must align with corporate policies\n\n---\n# Oracle University: Creating OCI Policies for Autonomous Dedicated\n\n**Instructor:** Linda Foinding  \n**Lesson Focus:** Managing roles, groups, and policies for Autonomous Dedicated Databases in OCI.\n\n---\n\n## 1. User Roles and Responsibilities\n\n| Role | Responsibilities | Permissions |\n|------|----------------|------------|\n| **Fleet Administrator** | - Manage Exadata infrastructure, Autonomous VM (AVM) clusters, and Autonomous Container Databases (ACD) <br> - Allocate budgets by compartments <br> - Provision Exadata infrastructure, AVM, ACD | Oracle Cloud user with permissions to manage resources and networking required for provisioning |\n| **Database Administrator (DBA)** | - Manage Autonomous Databases <br> - Create/manage Oracle Database users <br> - Provide access info to developers | Oracle Cloud user with permissions for Autonomous Database, backups, ACD, and networking |\n| **Database Users / Developers** | - Write applications connecting to Autonomous Databases <br> - Access data via credentials from DBA | Do not require Oracle Cloud accounts; rely on network connectivity & credentials from DBA |\n\n---\n\n## 2. Life Cycle Management\n- Can be performed via **OCI Console, CLI, REST APIs, SDKs** (Java, Node, Python, Go).  \n- Operations include:\n  - Create/delete/start/stop Exadata AVM and ACD  \n  - Create/delete/start/stop Autonomous Databases  \n  - Scale CPU, storage, or other resources  \n  - Backup/restore to point-in-time or long-term backups  \n  - Schedule updates for Exadata infrastructure, AVM, and ACD  \n  - Monitor via OCI monitoring and logging services  \n\n---\n\n## 3. Policies in OCI\n- Fine-grained control via **policies applied to groups**.  \n- **Resources:** Exadata infrastructure, Autonomous VM clusters, Autonomous Container Databases, Autonomous Databases, backups, data archives.  \n- **Policy Syntax:**  \n```Allow group <GROUP> to <VERB> <RESOURCE> in compartment <COMPARTMENT>```\n\n- **Verbs & Access Levels:**\n\n| Verb | Description |\n|------|------------|\n| Inspect | Limited read-only; for auditors |\n| Read | Read-only access to see details of resources |\n| Use | Full actions on existing resources |\n| Manage | Full access including create, delete, modify |\n\n---\n\n## 4. Sample Policy Structure\n- **Groups:**\n- **AcmeFA** \u2192 Fleet Administrators  \n- **RoadrunnerDBA / CoyoteDBA** \u2192 Developers & DBAs  \n- **Access Rules:**\n- AcmeFA: Manage, create, delete, and use Exadata infrastructure, AVM clusters, ACD in FA compartment.  \n- RoadrunnerDBA / CoyoteDBA: Read access to ACD in FA compartment; can create/manage Autonomous Databases in those ACDs.  \n\n```ResourceNames: autonomous-databases, autonomus backups```\n\n```Fleet Admin: cloud-exadata-infrastructure, cloud-autonomous-vmclusters, autonomous-container-databases```\n\n**Key Takeaway:**  \n- Fleet administrators control infrastructure and high-level resources.  \n- DBAs and developers manage databases and applications within their compartments while respecting constraints set by Fleet Admins.\n---\n# Oracle University: Monitoring Dedicated Infrastructure\n\n**Instructor:** Linda  \n\nAutonomous Database automates many DBA tasks, but **application DBAs** still need to monitor and diagnose databases to ensure **performance** and **security**.\n\n---\n\n## Key Responsibilities of Application DBAs\n- Perform operations on databases  \n- Clone and move databases  \n- Monitor performance and capacity  \n- Create alerts  \n- Perform low-level diagnostics for application performance  \n- Identify trends in usage and capacity  \n\n---\n\n## Tools Available\n- **Enterprise Manager**  \n- **Performance Hub**  \n- **OCI Console**  \n\n---\n\n## Autonomous Database Dedicated (ADB-D)\nAll operations are available via:\n- **Console UI**  \n- **REST API**  \n\n### Supported Operations\n- Provisioning  \n- Start/stop lifecycle management  \n- On-demand backups & restores  \n- CPU scaling and storage management  \n- Connectivity setup (wallets)  \n- Scheduling updates  \n\n---\n\n## Deep-Dive Exploration Tools\n- **Database Actions**  \n- **Performance Hub**  \n- **Enterprise Manager**  \n\n---\n\n## Additional Monitoring Resources\n- Oracle Documentation for:  \n  - **Resource usage tracking & visualization**  \n  - **Database Management services**  \n  - **Operations Insights**  \n  - **Events & notifications monitoring**  \n\n---\n\n**Conclusion:**  \nApplication DBAs play a vital role in ensuring optimal performance and security by leveraging Oracle's monitoring tools and documentation.\n---\n# OCI Exam Notes: ADB-D Maintenance Scheduling\n\n## Key Concepts\n- **Autonomous Database Dedicated (ADB-D)** allows **custom patching schedules**.  \n- Customers control:\n  - **Quarter** \u2192 which month  \n  - **Month** \u2192 which week  \n  - **Week** \u2192 which day  \n  - **Day** \u2192 which patching window  \n- Schedule can be **changed dynamically** if needed.\n\n## Best Practices\n- Patch flow: **Dev \u2192 Staging \u2192 Production**.  \n- Dev can run latest, Prod can lag behind (alternate quarters).  \n- Different DBs (e.g., mission-critical) can have **unique schedules**.  \n\n## Maintenance Options\n- Independent scheduling for **infrastructure, cluster, and container DB**.  \n- Options: **skip, reschedule, patch now**.  \n- Transparent application continuity built-in.  \n\n## Updates & Notifications\n- Updates released **quarterly**.  \n- Shown in **OCI Console** + **OCI Notifications**.  \n- Fleet admins may run **different patch levels** across environments.  \n\n## One-Off Patching\n- Outside normal cycle.  \n- Customer can **initiate patch now** or **reschedule**.  \n- Events/notifications track schedule, reminders, start, and end.\n\n---\n**Exam Tip:** Remember that **ADB-D = customer-controlled scheduling**, unlike shared ADB where Oracle controls patching.  \n\n---\n1. Actions in ACD details page:\n   * restart the CDB\n   * chagne the backup retention policy for the CDB\n   * move the CDB to a different compartment\n2. Inputs for provisioning autonomous exadata infra resource\n   * shape\n   * VCN\n   *  Availability domain\n3. which three levels can you implement isolation with ADD-D?\n   * VCN level\n   * container level\n   * database level \n---\n\n",
    "category": "SQL Server Administration",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "9 min read",
    "file": "Posts_Curated/sql-server-administration.md"
  },
  {
    "id": 9,
    "title": "SQL Server Automation: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to sql server automation, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"SQL Server Automation: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"SQL Server Automation\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to sql server automation, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n## IaaC: Provision the cloud infrastructure with code.\n\n### ARM Templates\n* Complete set of resoruce using single deployment template.\n* Declarative methods.\n* Extensibility, allowing you to run powershell or bash scripts on your resources post-deployment.\n\n### Powershell and Azure CLI (imperative model): follows the sequence of tasks to be executed.\n* With AZ module, provides commandlet\n* Az.Compute: Azure VMs\n* can deploy ARM templates.\n\n### AZURE CLI\n* Deploy or modify azure resources, some commands are for azure postgresql and azure mysql databases are only CLI.\n\n### Azure Portal:\n* UI for ARM.\n* Export template in the automation section\n\n### Azure Devops services\n* Azure Pipelines (build, test, deploy) \n* Deploy by powershell or by defining tasks that stage your artifacts and then deploy templates.\n* CI focus on small frequent changes to code and check VCS and CD focus automating the delivery of code change to underlying infrastrucrure\n\n# Automate Deployment using Azure Resource Manager Template and Bicep\n* ARM template are JSON documents that describe the resource to deploy within an Azure Resource Group.\n* Declarative\n* Orchestration: interdependent resources.\n## Benefits:\n* Repeatable\n* Orchestration\n* modular\n* exportable code\n* Authoring tools\n## Deploy with powershell\n### Template:\n1. $Schema\n2. contentVersion\n3. parameters\n4. resources.\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2021-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"serverName\": {\n      \"type\": \"string\"\n    },\n    \"sqlDBName\": {\n      \"type\": \"string\"\n    },\n    \"location\": {\n      \"type\": \"string\"\n    },\n    \"administratorLogin\": {\n      \"type\": \"string\"\n    },\n    \"administratorLoginPassword\": {\n      \"type\": \"secureString\"\n    }\n  },\n  \"resources\": [\n    {\n      \"type\": \"Microsoft.Sql/servers\",\n      \"apiVersion\": \"2022-02-01\",\n      \"name\": \"[parameters('serverName')]\",\n      \"location\": \"[parameters('location')]\",\n      \"properties\": {\n        \"administratorLogin\": \"[parameters('administratorLogin')]\",\n        \"administratorLoginPassword\": \"[parameters('administratorLoginPassword')]\"\n      }\n    },\n    {\n      \"type\": \"Microsoft.Sql/servers/databases\",\n      \"apiVersion\": \"2022-02-01\",\n      \"name\": \"[format('{0}/{1}', parameters('serverName'), parameters('sqlDBName'))]\",\n      \"location\": \"[parameters('location')]\",\n      \"dependsOn\": [\n        \"[resourceId('Microsoft.Sql/servers', parameters('serverName'))]\"\n      ]\n    }\n  ]\n}\n```\n#### Deploy with powershell\n```ps1\n$projectName = Read-Host -Prompt \"Enter the project\"\n$location = Read-Host -Prompt \"enter location\"\n$adminUser = Read-Host -Prompt \"Enter the SQL server administrator username\"\n$adminPassword = Read-Host -Prompt \"Enter the SQL server administrator password\" -AsSecureString\n\n$resourceGroupName = \"${projectName}rg\"\n\n# Create a new resource group\nNew-AzResourceGroup -Name $resourceGroupName -Location $location\n\n# Deploy resources using an ARM template\nNew-AzResourceGroupDeployment -ResourceGroupName $resourceGroupName -TemplateUri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/quickstarts/microsoft.sql/sql-database/azuredeploy.json\" -administratorLogin $adminUser -administratorLoginPassword $adminPassword\n```\n\n## Bicep is new delclerative language for deployeing azure resources. IaC tools.\n### Benefits:\n1. Continuous full support\n2. Simple syntax\n3. Easy to use.\n\n```bicep\nparam location string = resourceGroup().location\nparam storageAccountName string = 'toylaunch${uniqueString(resourceGroup().id)}'\n\nresource storageAccount 'Microsoft.Storage/storageAccounts@2022-09-01' = {\n  name: storageAccountName\n  location: location\n  sku: {\n    name: 'Standard_LRS'\n  }\n  kind: 'StorageV2'\n  properties: {\n    accessTier: 'Hot'\n  }\n}\n```\n### Azure free tire deployment with bicep\n```\n@description('Name of the SQL Server')\nparam serverName string\n\n@description('SQL administrator username')\nparam administratorLogin string\n\n@secure()\n@description('SQL administrator password')\nparam administratorPassword string\n\n@description('SQL Database name')\nparam databaseName string = 'SampleDB'\n\n@description('Location for all resources')\nparam location string = resourceGroup().location\n\nresource sqlServer 'Microsoft.Sql/servers@2022-02-01' = {\n  name: serverName\n  location: location\n  properties: {\n    administratorLogin: administratorLogin\n    administratorLoginPassword: administratorPassword\n  }\n}\n\nresource sqlDb 'Microsoft.Sql/servers/databases@2022-02-01' = {\n  name: '${serverName}/${databaseName}'\n  location: location\n  sku: {\n    name: 'Basic'      // Basic = 5 DTUs, 2 GB\n    tier: 'Basic'\n    capacity: 5\n  }\n  dependsOn: [\n    sqlServer\n  ]\n}\n```\n```ps1\nNew-AzResourceGroup -Name exampleRG -Location eastus\nNew-AzResourceGroupDeployment -ResourceGroupName exampleRG -TemplateFile ./main.bicep -administratorLogin \"<admin-login>\"\n```\n### Main sections\n1. param: param location string = resourceGroup().location\n2. var: var fullDbName = '${serverName}-${databaseName}'\n3. resource: resource sqlDb 'Microsoft.Sql/serers/database@2022-02-01' = {...}\n4. Module: reuse logic by referencing other bicep files Module vnetModule './vnet.bicep' = {...}\n4. output: output sqlServerName string = sqlServer.name\n5. targetScope: define scope of deployement targetScope = 'resourceGroup'\n## Source control for templates\n1. use the deploy to azure in github pages for sql database templates, the tempate load and fill in few details like resource group, location and admin credential.\n\n\n# Automate deployment by using powershell\n* supports both text and .NET objects\n* Az.Sql powershell module part of Az powershell module\n* from creating database to configuring geo-replciation and full azure sql amangement\n* Az.Sql PowerShell module in various environments, including PowerShellGet, Azure Cloud Shell, and an Az PowerShell Docker container\n* the verb-noun structure.\n    * <command-name> -<Required Parameter Name> <Required param value> [-<Optional paramer name> <operational param value>]\n    * Create resource group: ```Get-AzSqlServer -ResourceGroupName \"ResourceGroup01\" -ServerName \"Server01\"```\n    * create amanaged instance: ```New-AzSqlInstance -Name managedInstance2 -ResourceGroupName ResourceGroup01 -Location westcentralus -AdministratorCredential (Get-Credential) -SubnetId \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/resourcegroup01/providers/Microsoft.Network/virtualNetworks/vnet_name/subnets/subnet_name\" -LicenseType LicenseIncluded -StorageSizeInGB 1024 -VCore 16 -Edition \"GeneralPurpose\" -ComputeGeneration Gen4```\n    * ```New-AzSqlDatabase -ResourceGroupName \"ResourceGroup01\" -ServerName \"Server01\" -DatabaseName \"Database01\"```\n\n```ps1\nNew-AzSqlInstance -Name managedInstance2 -ResourceGroupName ResourceGroup01 -ExternalAdminName DummyLogin -EnableActiveDirectoryOnlyAuthentication -Location westcentralus -SubnetId \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/resourcegroup01/providers/Microsoft.Network/virtualNetworks/vnet_name/subnets/subnet_name\" -LicenseType LicenseIncluded -StorageSizeInGB 1024 -VCore 16 -Edition \"GeneralPurpose\" -ComputeGeneration Gen4\n\n$val = Get-AzSqlInstance -Name managedInstance2 -ResourceGroupName ResourceGroup01 -ExpandActiveDirectoryAdministrator\n```\n\n# deploy with Azure CLI\n```az account set --subscription \"my subscription name\"```\n\n\n|Command| Azure CLI|Azure PowerShell|\n|---|---|---|\nSign in with Web Browser|\taz login|\tConnect-AzAccount\nGet available subscriptions|\taz account list\t|Get-AzSubscription\nSet Subscription|\taz account set \u2013subscription|\tSet-AzContext -Subscription\nList all virtual machines\t|az vm list\t|Get-AzVM\nCreate a new SQL server\t|az sql server create\t|New-AzSqlServer\n\n```Invoke-Sqlcmd```: to invoke the command\n\n```\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\n\n$resourceGroup = \"<your resource group>\"\n$location = \"<your location preference>\"\n$server = \"dp300-sql-server-$randomIdentifier\"\n$login = \"sqladmin\"\n$password = \"Pa$$w0rD-$randomIdentifier\"\n\naz sql server create --name $server --resource-group $resourceGroup --location \"$location\" --admin-user $login --admin-password $password\n\naz sql server firewall-rule create --resource-group $resourceGroup --server $server -n AllowYourIp --start-ip-address 0.0.0.0 --end-ip-address 0.0.0.0\n```\n\n## Deploy ARM template with azure CLI and powershell\n```ps1\nNew-AzResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName ExampleResourceGroup -TemplateFile c:\\MyTemplates\\azuredeploy.json -TemplateParameterFile c:\\MyTemplates\\storage.parameters.json\n```\n\n```bash\naz deployment group create --resource-group ExampleResourceGroup --template-file '\\path\\template.json'\n```\n\n* Currently, Azure CLI doesn't support deploying remote Bicep files directly. Instead, you can use the Bicep CLI to convert the Bicep file into a JSON template, and then deploy the JSON template from a remote location.\n\n\n",
    "category": "SQL Server Automation",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "5 min read",
    "file": "Posts_Curated/sql-server-automation.md"
  },
  {
    "id": 10,
    "title": "SQL Server High Availability: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to sql server high availability, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"SQL Server High Availability: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"SQL Server High Availability\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to sql server high availability, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# Explore IaaS and PaaS solutions for high availability and disaster recovery\n* Understand an AG you need to understand how deploying a windows server failover cluster (WSFC), \n    * In IaaS, the configuration managed at the azure level.\n    * In PaaS, its in database or database server itself.\n\n## Describe failover clusters in windows server\n* Witness resource, for the quorum mechanism\n    * can be disk, file share (SMB 2.0 or later) or cloud witness. \n* MS Distributed transaction coordinator (DTC or MSDTC), those that do it to be clustered if deploying an AG or FCI. \n* Workgroup cluster: The WSFC itself needs a unique name in the domain (and DNS) and requires an object in AD DS called the cluster name object (CNO).\n*A typical Azure-based WSFC will only require a single virtual network card (vNIC). Unlike on-premises setups, the IP address for the vNIC must be configured in Azure, not within the VM. Inside the VM, it appears as a dynamic (DHCP) address, which is expected. However, cluster validation generates a warning that can be safely ignored.*\n\n\n1. Enable failover clustering\n```Install-windowsfeature failover-clustering -includemanagementtools```\n2. Validate\n    * *For FCIs, these tests also check the shared storage to ensure that the disks are configured correctly. For AGs with no shared storage, in Windows Server 2016 and later, the results come back as not applicable. For Windows Server 2012 R2, the disk tests show a warning when there are no shared disks. This state is expected.*\n3. Create the new cluster\n```New-Cluster -Name MyWSFC -Node Node1,Node2,\u2026,NodeN -StaticAddress w.x.y.z```\nfor the workgroup cluster that only use DNS, syntax: ```New-Cluster -Name MyWSFC -Node Node1,Node2,\u2026,NodeN -StaticAddress w.x.y.z -NoStorage -AdministrativeAccessPoint DNS```\n```New-Cluster -Name MyWSFC -Node Node1,Node2,\u2026,NodeN -StaticAddress w.x.y.z -NoStorage -ManagementPointNetwork Singleton```\n#### MIltiple subnet\n* Virtual network names on DNS server are updated with virtual IP addresses corresponding to the respective subnets\n* After a multi-subnet failover, clients and applications can then connect to the FCI via the same virtual network name.\n\n## COnfigure Always on\n* necessary to have identical storage configurations.\n* enable feature. \n* Create the availability group\n* Create an internal azure load balancer\n\n\n* Use of the Load balancer\n* probe port. Load balancer required unique probe port: Get-ClusterResource IPAddressResourceNameForListener | Set-ClusterParameter ProbePort PortNumber \\\n```Test-NetConnection NameOrIPAddress -Port PortNumber```\n* The main difference between an on-premises configuration and an Azure configuration for a distributed AG is that as part of the load balancer configuration in each region, the endpoint port for the AG needs to be added. The default port is 5022.\n\n### azure site recovery:\n* There's a Site Recovery Mobility extension configured on the VM.\n* Changes are sent continually unless Azure Site Recovery is unconfigured or replication is disabled\n* Crash consistent recovery points are generated every five minutes, and application-specific recovery points are generated according to what is configured in the replication policy.\n--- \n* Volume Shadow Service (VSS) \u2013 lowering this value could potentially cause problems for SQL Server since there's a brief freeze and thaw of I/O when the snapshots are taken. The impact of the freeze and thaw could be magnified if other options such as an AG are configured. Most won't encounter issues, but if Azure Site Recovery interferes with SQL Server, you may want to consider other availability options.\n\n* A consideration for Azure Site Recovery is if there's a failover to another region, the replica VMs aren't protected when they're brought online. They'll have to be reprotected\n---\n### Active geo replication\n* Active geo-replication provides business continuity by allowing customers to programmatically or manually failover primary databases to secondary regions during major disaster.\n* Azure SQL Managed Instance doesn't support active geo-replication. You must use auto-failover groups instead, which will learn on the next unit.\n* the geo-secondary is configured with the same compute size as the primary\n* multiple replicas\n* Async\n* any region\n* manual failvoer only.\n\n### Cross subscription geo-replication is a feature that allows you to perform this task.\n\n### Auto-failover group:\n* Uniuq within the domain. SQL MI supports one auto-failover group.\n* Has listner: (no need to update connection string after failover)\n* different region\n* primary logical server and secondary logical server\n    * Automatic: switches to region\n    * Read-ony: read-only listner is disabled to ensure performance of the new primary when the secondary is down. \n* configure GradePeriodWithDataLossHours: how long azure waits before failing over. no dataloss set higher, more time to sync \n* one or more database ( same size and edition.) seeding. \n\nBoth supports read scale\n\n### Setup:\n* Do you need long-term backups? Or is 1-35 days long enough?\n* What are your RTO and RPO needs?\n* Based on the SLA, what service tier makes the most sense?\n* Do you need Availability Zones?\n* Do you need geo-replicated HADR or failover groups?\n* Is your application ready?\n---\n#### Monitor:\n1. Azure status\n2. Azure Service Health\n---\n* az sql mi list lists the status of managed instances.\n* az sql db list lists the status of Azure SQL databases.\n\nYou can also use PowerShell commands to determine the availability of an Azure SQL database. For example:\n* Get-AzSQLDatabase gets all the databases on a server and their details, including status.\n* REST APIs aren't as easy to use, but you can use them to get the status of managed instances and databases.\n\n## Backup history:\n* backup database and txn logs\n* XEvents to track hisotry\n* log view\n\n### Replica Status\n```sys.dm_database_replica_states```\n\n* resource health by using the Azure portal or REST APIs.\n",
    "category": "SQL Server High Availability",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "5 min read",
    "file": "Posts_Curated/sql-server-high-availability.md"
  },
  {
    "id": 11,
    "title": "SQL Server Performance Tuning: Complete Guide",
    "excerpt": "A comprehensive, production-ready guide to sql server performance tuning, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.",
    "content": "---\ntitle: \"SQL Server Performance Tuning: Complete Guide\"\ndate: \"2026-01-12\"\ncategory: \"SQL Server Performance Tuning\"\ntags: []\nexcerpt: \"A comprehensive, production-ready guide to sql server performance tuning, covering fundamentals, best practices, troubleshooting, and real-world examples from enterprise environments.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n# The modern SQL Server DBA - HandBook [Interview Prep]\nIn the age of LLM where you can buy memory on cheap price, the deciding factors for capable employee has drastically changed. Unlike the technical requirements, it has been necessary to analyse the phychological and behavioural traits of a person.\n* Stress Innoculation and Cognitive performance under stress: Reaction and approach during the production-down scenario.\n* Root cause analysis and systematic thinking:\n* Intellectual humility and a growth mindset: Open about their mistakes and what they learned from them. How to stay current to rapidly evolving tech?\n* Ownership and Accountability: We vs I when describing successes and failures. \n* Leadership and mentoring propensity: How do they explain complex topics? Articulate a vision.\n\n### ARIES (Algorithm for Recovery and Isolation Exploiting Semantics): It is a recovery algorithm. ARIES ensures that committed transactions are durable and that incomplete transactions are rolled back during recovery. Achieves through WAL, fuzzy checkpointing (LSN), and redo/undo operations.\n1. Analysis Phase: Last successful checkpoint to identify dirty pages in the buffer pool at the time of the crash and the active transactions. Start of redo phase. determine ```redoLSN```: point from which redo should begin.\n2. Redo Phase: Starting from the determined point (min(recoveryLSN)) of dirty pages. All logging changes (redoes). \n3. ARIES rolls back the changes made by tXN that were incomplete or uncommitted at the time of the crash. Log record in reverse LSN order. Undo log CLRs can be tracked.\n\n### ADR: It uses \n* ```Persistent Version Store (PVS)``` Maintains the version store directly in the user database: tracks row level changes, access previous version of data.\n* Logical Revert: Instant revert using the PVS.\n* sLog: A secondary in-memory log stream that stores log records for non-versioned operations. Only process these small operations quickly in redo and undo. \n## Interview Preperation\n### 1. Describe troubleshooting a slow-running query reported by a user:\n    * Try reproducing the issue\n    * capture exection plan (actual and estimated)\n        * ```set statistics IO on;```\n        * ```set statistics TIME on;```\n        * Look for anything like Table scans taking high cost and long time or missing indexes or warnings like implicit conversions or memory spills.\n    * check waits (```sys.dm_os_wait_stats```):\n        * CXPACKET: parallelism tuning\n        * PAGEIOLATCH: io bottleneck\n        * LCK_M_IX: insert/update locking\n    * review indexes and stats on involved tables\n        * Are the used column indexed\n        * use ```exec sp_helpindex 'tablename';```\n    * check for blocking/locks\n        * check if there is any blocking ```sys.dm_tran_locks```\n    * Validate parameter sniffing, implicit conversions, or bad query patterns\n        * Try with different ```@OrderDate``` values.\n        * Use ```option(recompile)``` to see if the plan changes.\n        * Set query store on.\n    * Tune query/indexes or rewrite as needed\n        * in some cases filtered index or for reporting query indexed view can be helpful. \n\n### 2. Difference between recompile and optimize for unknown\n\n|recompile | Optimize for unknown|\n|----------|---------------------|\n|fresh plan for each exection(parameter-sensative query)| ignores specific parameters values; uses average distribution (avoid parameter sniffing)|\n```SQL\nSELECT COUNT_BIG(*)\nFROM Sales.SalesOrderDetail sod\nJOIN Sales.SalesOrderHeader soh ON sod.SalesOrderID = soh.SalesOrderID\nWHERE sod.LineTotal > 0;\n-- OPTION (Optimize for unknown);\n-- OPTION (recompile);\n```\n### 3. Common wait status and Prameters\n| parameters | meaning| values|\n|--|--|--|\n|CXPACKET|there are often multiple threads in the query execution (producer and consumer). CXPACKET is measure of time these threads spends waiting to sync and exchange data with each other. High CXPACKET doesn't always means its bad. In OLAP with processors and large queries, its expected. Skewed parallelism. Uneven data distribution.(Cause: missing index, MAXDOP, Cost threshold for parallelism, out of stat, CPU pressure, IO bottleneck| CTFP (cost threshold for parallelism) is 5 by default, raise to 20,50 for OLTP. CXPACKET should be non-existant or low. ```high cxpacket + low cxconsumer, both high``` &rarr; bad\n|PAGEIOLATCH | Shared, Exclusive, Update &rarr; waiting for the page data loaded from disk into the buffer pool or change to a page in memory to be completed. | Always be low for OLTP. Poor disk latency. Use PerfMon (Windows Performance Monitor)\n|LCK_M_XX| shared lock, exclusive lock, update lock: process waiting to acquire a lock on a resource (row, page, object, database) | even minor affects\n|SOS_SCHEDULER_YIELD| Thread has yielded the cpu to allow other threads to execure. Busy scheduler. |\n|WriteLOG| when txn commits and sql server has to wait for the txn log records to be physcially flushed to the txn log file on disk. | should be very low\n| ASYNC_NETWORK_IO | server has sent data to the client application, but waiting for the acknowledgement. | High fast server but network or client is dead\n\n```SQL\nSELECT \n    [Wait_Type],\n    [Wait_Time_MS] / 1000.0 AS [Wait_Time_Seconds],\n    [Waiting_Tasks_Count],\n    [Wait_Time_MS] * 1.0 / NULLIF([Waiting_Tasks_Count], 0) AS [Avg_Wait_Time_MS],\n    CASE [Wait_Type]\n        WHEN 'CXPACKET' THEN 'Too much parallelism or bad query plan; tune MAXDOP and cost threshold'\n        WHEN 'PAGEIOLATCH_SH' THEN 'Slow I/O subsystem; improve storage or tune queries'\n        WHEN 'PAGEIOLATCH_EX' THEN 'Heavy write activity or contention; consider batching or faster disks'\n        WHEN 'LCK_M_IX' THEN 'Blocking caused by concurrent writes; tune indexing or isolation levels'\n        ELSE 'Other wait type \u2013 check official documentation'\n    END AS [Interpretation]\nFROM sys.dm_os_wait_stats\nWHERE [Wait_Type] IN (\n    'CXPACKET',\n    'PAGEIOLATCH_SH',\n    'PAGEIOLATCH_EX',\n    'LCK_M_IX',\n\t'CXCONSUMER'\n)\nORDER BY [Wait_Time_MS] DESC;\n\n-- ReadwriteIO\nSELECT DB_NAME(database_id) AS db_name, file_id,\n       io_stall_read_ms / NULLIF(num_of_reads, 0) AS avg_read_ms,\n       io_stall_write_ms / NULLIF(num_of_writes, 0) AS avg_write_ms\nFROM sys.dm_io_virtual_file_stats(NULL, NULL);\n\n-- Missing index:\nSELECT TOP 10 *\nFROM sys.dm_db_missing_index_details\nORDER BY avg_total_user_cost DESC;\n```\n## Checking for the LOG FLUSH: \n```SQL\nSELECT \n    instance_name AS database_name,\n    counter_name,\n    cntr_value\nFROM sys.dm_os_performance_counters\nWHERE counter_name IN ('Log Bytes Flushed/sec', 'Log Flush Wait Time')\n  AND object_name LIKE '%Databases%'\nORDER BY instance_name;\n\n-- \nSELECT TOP 10\n    wait_type,\n    wait_time_ms / 1000.0 AS total_wait_seconds,\n    waiting_tasks_count AS wait_count,\n    wait_time_ms / NULLIF(waiting_tasks_count, 0) AS avg_wait_time_ms,\n    100.0 * wait_time_ms / SUM(wait_time_ms) OVER() AS percent_of_total\nFROM sys.dm_os_wait_stats\nWHERE wait_type NOT IN (\n    'CLR_SEMAPHORE','LAZYWRITER_SLEEP','RESOURCE_QUEUE','SLEEP_TASK',\n    'SLEEP_SYSTEMTASK','SQLTRACE_BUFFER_FLUSH','WAITFOR','LOGMGR_QUEUE',\n    'CHECKPOINT_QUEUE','REQUEST_FOR_DEADLOCK_SEARCH','XE_TIMER_EVENT','BROKER_TO_FLUSH',\n    'BROKER_TASK_STOP','CLR_MANUAL_EVENT','CLR_AUTO_EVENT','DISPATCHER_QUEUE_SEMAPHORE',\n    'FT_IFTS_SCHEDULER_IDLE_WAIT','XE_DISPATCHER_WAIT','XE_DISPATCHER_JOIN',\n    'SQLTRACE_INCREMENTAL_FLUSH_SLEEP','BROKER_EVENTHANDLER','TRACEWRITE',\n    'WAIT_XTP_HOST_WAIT','HADR_FILESTREAM_IOMGR_IOCOMPLETION',\n    'HADR_WORK_QUEUE','HADR_LOGCAPTURE_WAIT','HADR_TIMER_TASK','HADR_WORK_QUEUE',\n    'HADR_SIGNAL_ACCESS','WAIT_FOR_RESULTS','BROKER_RECEIVE_WAITFOR','PREEMPTIVE_OS_GETPROCADDRESS',\n    'PREEMPTIVE_OS_AUTHENTICATIONOPS','PREEMPTIVE_OS_LIBRARYOPS','PREEMPTIVE_OS_COMOPS',\n    'PREEMPTIVE_OS_WAITFORSINGLEOBJECT','PREEMPTIVE_OS_AUTHORIZATIONOPS'\n)\nORDER BY wait_time_ms DESC;\n\nDBCC SQLPERF('sys.dm_os_wait_stats', CLEAR);\n```\n### 4. How do you resolve the blocking and deadlocking?\n> sys.dm_tran_locks, sp_who2, sys.dm_exec_requests, Extended events. Find the blocker, check for slowness, review indexing and adjust isolation, SI or RCSI. \n```SQL\n-- Find blocking session:\nSELECT \n    r.session_id AS blocked_session_id,\n    r.status,\n    r.blocking_session_id,\n    r.wait_type,\n    r.wait_time,\n    r.wait_resource,\n    s.login_name,\n    s.host_name,\n    t.text AS blocked_query\nFROM sys.dm_exec_requests r\nJOIN sys.dm_exec_sessions s ON r.session_id = s.session_id\nCROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t\nWHERE r.blocking_session_id <> 0;\n\n-- Head blockers (sessions not blocked by others but blocking others)\nSELECT \n    session_id,\n    login_name,\n    host_name,\n    status,\n    cpu_time,\n    memory_usage,\n    program_name\nFROM sys.dm_exec_sessions\nWHERE session_id IN (\n    SELECT blocking_session_id\n    FROM sys.dm_exec_requests\n    WHERE blocking_session_id <> 0\n)\nAND session_id NOT IN (\n    SELECT session_id\n    FROM sys.dm_exec_requests\n    WHERE blocking_session_id <> 0\n);\n\nSELECT \n    request_session_id AS session_id,\n    resource_type,\n    resource_database_id,\n    resource_associated_entity_id,\n    request_mode,\n    request_status\nFROM sys.dm_tran_locks\nWHERE request_status = 'WAIT';\n```\n#### ISOLATION TYPES\n---\n|Type | Define | USE|usage|\n|---|---|---|---|\nREAD UNCOMMITTED| dirty, non-repeatable, phantom | It can just read anything like commited uncommited doesn't care | No blocking atall. read row even when another txn hasn't committed.\nCOMMITTED| no dirty, non-repeatable, phantom | You will see the data in your txn once other session commits it with ending your txn | Uses shared locks only during read, other session can still write afterwareds. Order entry, inventory lookup.\nREPEATABLE READ| only phantom | prevent changes to rows read. Only to the single rows | Prevent other rows from updating or deleting these rows, loan approval\nSERIALIZABLE| Full isolation | prevent anything selected between the range of rows. like it can be >1000 | prevent insert, updates or deletes on any row. protect against phantom read\nSNAPSHOT| full isolation | High concurrency with consitency. It will just show you a version. Once you start your txn, it a different version of database at that state for you | System with high concurrency small OLTP with complex analytics like hospital system.\n\n>Dirty Read:\tReading data that is uncommitted and may roll back\\\n>Non-Repeatable Read:\tA row value changes between two reads in the same transaction\\\n>Phantom Read:\tNew rows are added or removed between reads, changing the result set\n\n> OPEN Transaction can prevent cleanup of older row versions in the tempdb version store (for snapshot isolation) or the persistent version store (PVS) for database.\n\n### Switch between READ COMMITTED AND SNAPSHOT\n#### READ COMMITTED &rarr; SNAPSHOT\n1. Read heavy OLTP systems: 90% reads. \n2. ETL or financial processes needing stable views.\n3. Frequent non-repeatable reads or phantom.\n4. Deadlocks in high-concurrency workloads.\n5. Frequent blocking during long reads. (reporting query blocked by OLTP writes)\n\n#### Shapshot &rarr; READ COMMITTED\n1. tempdb Pressure/ version store overflow (high update rates and long-running readers)\n2. Non need for repeatable reads or phantom protection.\n3. Reduce disk i/o for small txn. \n4. Conflicts between multiple writers: writers using snapshot can cause update conflicts. \n5. Third-party apps that don't handle snapshot isolation conflict errors.\n\n### 5 Parameter Sniffing\nSometimes the plan is made using the first parameter used which can be good for common values but bad for outliers.\n>FIXES: ```optimise for, recompile```, plan guides, or rewrite procedure.\\\nOptimise for unknown: use avg distribution whereas recomile genrates new plan for the value\n\n### 6. Clustered and non-clustered index\n* Clustered: Physcial index on any key or combination of keys/columns. ONly one.\n    * If you create ```primary key``` without specifying the nonclustered, server will create by default. Incrementing rows are ideal.\n* non-clustered: Logical index with pointer to actual rows. they include clustered index key in their leaf level.\n    * Can be multiple. Can do with ```include()```\n    * This is called covering index.\n* Unique index (clustered or non-clustered): Enforce uniqueness of the value in index key\n* Flitered index (non-clustered): includes a where clause to filter the data.\n* Columnstore index: For OLAP, data warehousing. Column-oriented format rather than row, compressed, and vastly imporves the aggregate queries, data warehousing query. \n* XML index (primary and secondary): optimize querying of XML data stored in XML data type columns. Primary XML index: shared the XML data into set of internal tables and secondary further optimise for query patterns like path, property, value.\n\n```SQL\n-- Base table for most examples\nCREATE TABLE Customers (\n    CustomerID INT IDENTITY(1,1) NOT NULL,\n    FirstName NVARCHAR(50) NOT NULL,\n    LastName NVARCHAR(50) NOT NULL,\n    Email NVARCHAR(100) NULL,\n    RegistrationDate DATETIME DEFAULT GETDATE(),\n    IsActive BIT DEFAULT 1\n);\n\n-- Base table for XML Index example\nCREATE TABLE ProductCatalog (\n    ProductID INT PRIMARY KEY,\n    ProductName NVARCHAR(100),\n    ProductDetails XML\n);\n\n-- Option A: Primary Key defaults to Clustered Index\nALTER TABLE Customers\nADD CONSTRAINT PK_Customers PRIMARY KEY NONCLUSTERED (CustomerID);\n\nALTER TABLE Customers\nDROP CONSTRAINT PK_Customers;\n\n\n-- Option B: Explicitly create a Clustered Index on a non-primary key column\n-- (If PK is NONCLUSTERED, or if no PK)\n-- This assumes CustomerID is NOT already the clustered index\n-- Example: If you frequently query by LastName ranges\nCREATE CLUSTERED INDEX CX_Customers_LastName\nON Customers (LastName ASC, FirstName ASC);\n\n\n-- Create a Non-Clustered Index on the Email column\nCREATE NONCLUSTERED INDEX IX_Customers_Email\nON Customers (Email ASC);\n\n-- This index can satisfy queries needing CustomerID, FirstName, LastName, and Email\n-- without going back to the base table (if CustomerID is the clustered key).\nCREATE NONCLUSTERED INDEX IX_Customers_EmailWithDetails\nON Customers (Email ASC)\nINCLUDE (FirstName, LastName, RegistrationDate);\n\n-- Create a Unique Non-Clustered Index on the Email column\n-- This prevents duplicate email addresses.\nCREATE UNIQUE NONCLUSTERED INDEX UQ_Customers_Email\nON Customers (Email ASC);\n\n-- If you wanted a unique clustered index, you'd specify CLUSTERED with UNIQUE\n-- (e.g., if you created a new table where a composite key defined physical order and uniqueness)\nCREATE TABLE UserLogins (\n    UserID INT NOT NULL,\n    LoginProviderID INT NOT NULL,\n    LoginDate DATETIME NOT NULL,\n    CONSTRAINT UQ_UserLogins UNIQUE CLUSTERED (UserID, LoginProviderID)\n);\n\n-- Create a Filtered Index on LastName, but only for active customers.\n-- This index will be smaller and faster for queries specifically on active customers.\nCREATE NONCLUSTERED INDEX IX_Customers_Active_LastName\nON Customers (LastName ASC, FirstName ASC)\nWHERE IsActive = 1;\n\n-- SELECT CustomerID, FirstName, LastName FROM Customers WHERE IsActive = 1 AND LastName = 'Smith';\n\n-- To convert an existing table to a Clustered Columnstore Index,\n-- you must first drop any existing clustered index (like PK_Customers from above)\nDROP index CX_Customers_LastName on Customers;\nDROP INDEX IF EXISTS IX_Customers_Active_LastName ON Customers;\nDROP INDEX IF EXISTS IX_Customers_Email ON Customers;\nDROP INDEX IF EXISTS IX_Customers_EmailWithDetails ON Customers;\nDROP INDEX IF EXISTS UQ_Customers_Email ON Customers;\n\n\n-- Then create the Clustered Columnstore Index.\n-- The table itself is now stored in a column-oriented format.\nCREATE CLUSTERED COLUMNSTORE INDEX CCI_Customers\nON Customers;\n\nCREATE NONCLUSTERED COLUMNSTORE INDEX NCCI_CustomersMinimal\nON Customers (Email);\n\n\n\n-- If uniqueness is still required\nCREATE UNIQUE NONCLUSTERED INDEX UQ_Customers_Email\nON Customers (Email);\n\n-- Optional narrow lookup\nCREATE NONCLUSTERED INDEX IX_Customers_Email\nON Customers (Email);\n\n-- Optional if filtering by active and sorting by name\nCREATE NONCLUSTERED INDEX IX_Customers_Active_LastName\nON Customers (LastName, FirstName)\nWHERE IsActive = 1;  -- assuming this is a real column\n\n\nexec sp_helpindex 'Customers';\n\n\nSELECT \n    i.name AS index_name,\n    c.name AS column_name\nFROM sys.indexes i\nJOIN sys.index_columns ic ON i.object_id = ic.object_id AND i.index_id = ic.index_id\nJOIN sys.columns c ON c.object_id = ic.object_id AND c.column_id = ic.column_id\nWHERE i.object_id = OBJECT_ID('dbo.Customers')\nAND i.type_desc = 'CLUSTERED COLUMNSTORE';\n```\n* Clustered Columnstore index (CCI): replaces the rowstore entirely and NON-clustered columnstore index (NCCI) sits alongside the rowstore. Hence, only one can be created for consistency.\n### Fragmantation in SQL server: Internal and External\n* Fragmantation: logical order of data doesn't equal to the physical order of data.\n* REORGANIZE: Online, defrag only (< 30% fragmentation)\n* REBUILD: recreated index( offline > 30%)\n>Internal: Wasted empty space within a data page or index page. Rows don't perfectly fill the page.\\\n>External: Logical(Index scan order) and physical (disk storage) order of pages being out of sync. Pages are not contiguous on disk leading to extra I/O\n```SQL\nSELECT\n    DB_NAME(ips.database_id) AS DatabaseName,\n    OBJECT_NAME(ips.object_id) AS TableName,\n    i.name AS IndexName,\n    ips.index_type_desc,\n    ips.alloc_unit_type_desc,\n    ips.index_depth,\n    ips.index_level,\n    ips.avg_fragmentation_in_percent, -- This is the key metric for external fragmentation\n    ips.fragment_count,\n    ips.page_count,\n    ips.avg_page_space_used_in_percent -- This indicates internal fragmentation\nFROM\n    sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'DETAILED') AS ips\nINNER JOIN\n    sys.indexes AS i ON ips.object_id = i.object_id AND ips.index_id = i.index_id\nWHERE\n    ips.avg_fragmentation_in_percent > 5 -- Filter for indexes with more than 5% fragmentation\n    AND ips.page_count > 100 -- Ignore very small indexes\nORDER BY\n    ips.avg_fragmentation_in_percent DESC;\n```\n>* FILLFACTOR: Amount of leafs to be filled with data. 100 means all data (chances of fragmantion on more inserts, less storage), 70-90 means more free space for future inserts but high amount of space and I/O during index scans.\\\n>* PAD_INDEX: Boolean value (ON or OFF): when pad_index is on the fill factor is applied to intermediate leavels of index's B-tree not just the leaf level. \n\n### 8. Covering indexing\n* It is a non-cluster index that covers all the columns using include. No lookup needed. It reduces I/O and improves read performance. \n\n### 9. Filtered index:\n* It is a non-clustered index targing the subset of row. \n\n### 10. Purpose of Query Store.\n* It saves the historical query executed along with query plan, runtime stat. As a whole full history.\n* Helps to identify regression, force good plans. \n* Used for performance tuning and trobuleshooting.\n\n### 11. Describe when to use Extended events to trouble shoot a perfomance issue. \n* Example: to generate a deadlock report using xml_deadlock_report\n* Captured event using XE session and analyzed details (locks, processes, victims)\n    * Long running queries: Events: sql_statment_completed, rpc_completed\n    * Filter: duration > x ms or CPU > y ms\n    * CPU time, reads, writes, duration, SQL text\n* Wait Info, wait_completed, wait_info_external\n    * understand bottleneck\n    * better than sys.dm_os_wait_stats.\n\n* TempDB contention:\n    * event: wait_info + page_allocated\n    * Excessive allocation contention. \n\n### Usage of TempDB:\n* User-defined temporary tables #localtemptable, ##globaltemptable\n* Internally use tempdb if memory spills occur @tableVar\n* Work tables: Sorts, joins, hash match (query plans that spills intermediate results.)\n* Select into #temp from.. Write into tempdb\n* Cursor: use tempdb for large datasets\n* Triggers: nested trigger operations. uses tempdb if recursion or resultset size requires it\n* Snapshot isolation, version store. \n* Online index rebuild\n* groupby order by\n* spooling operators (lazy spools, eager spool)\n* DBCC Checkdb checktable\n* service broker\n* caching\n* Parallelism\n* XE (extended events/ring buffers)\n* QUERY STORE (tempspace during flishing to disk.)\n---\nBest Practice\n* Add multiple data files to reduce allocation contention (1 file per logical CPU upto 8)\n* Place tempdb on SSD (fast storage) instead of SAN disk with high throughput\n* Use trac flat 1117/1118 or rely on auto-uniform extent allocations.\n* Monitor with wait stats, perfmon, and XE.\n---\n1. filesize and space usage\n2. internal/user/version store page\n3. session-wise usage\n4. tempdb-related waits\n\n```SQL\nSELECT \n    s.session_id,\n    r.status,\n    r.command,\n    t.text AS sql_text,\n    tsu.internal_objects_alloc_page_count,\n    tsu.user_objects_alloc_page_count\nFROM sys.dm_exec_requests r\nJOIN sys.dm_exec_sessions s ON r.session_id = s.session_id\nJOIN sys.dm_db_task_space_usage tsu ON r.request_id = tsu.request_id\nOUTER APPLY sys.dm_exec_sql_text(r.sql_handle) t\nWHERE r.database_id = 2;\n---\nUSE msdb;\nGO\n\nIF NOT EXISTS (SELECT * FROM tempdb.sys.tables WHERE name = 'TempDB_Monitor_Log')\nBEGIN\n    CREATE TABLE dbo.TempDB_Monitor_Log (\n        LogTime DATETIME DEFAULT GETDATE(),\n        FileSizeMB INT,\n        UsedSpaceMB INT,\n        InternalObjectsMB INT,\n        UserObjectsMB INT,\n        VersionStoreMB INT,\n        PagelatchWaits BIGINT,\n        TopSessionID INT,\n        TopSessionCommand NVARCHAR(100),\n        TopSessionMBUsed INT\n    );\nEND\n\n---SQL Agent job\nUSE msdb;\nGO\n\nEXEC sp_add_job @job_name = N'TempDB Monitoring Job';\n\nEXEC sp_add_jobstep\n    @job_name = N'TempDB Monitoring Job',\n    @step_name = N'Capture TempDB Metrics',\n    @subsystem = N'TSQL',\n    @command = N'\nDECLARE @FileSizeMB INT, @UsedSpaceMB INT, @InternalMB INT, @UserMB INT, @VersionMB INT;\nDECLARE @PagelatchWaits BIGINT, @TopSessionID INT, @TopSessionCommand NVARCHAR(100), @TopSessionMBUsed INT;\n\n-- File size\nSELECT @FileSizeMB = SUM(size) * 8 / 1024\nFROM sys.master_files\nWHERE database_id = 2;\n\n-- Used space\nEXEC tempdb..sp_spaceused;\nSELECT @UsedSpaceMB = SUM(unallocated_extent_page_count + version_store_reserved_page_count\n  + user_object_reserved_page_count + internal_object_reserved_page_count) * 8 / 1024\nFROM tempdb.sys.dm_db_file_space_usage;\n\n-- Internal and User pages\nSELECT \n    @InternalMB = SUM(internal_objects_alloc_page_count) * 8 / 1024,\n    @UserMB = SUM(user_objects_alloc_page_count) * 8 / 1024\nFROM tempdb.sys.dm_db_task_space_usage;\n\n-- Version store\nSELECT @VersionMB = SUM(version_store_reserved_page_count) * 8 / 1024\nFROM tempdb.sys.dm_db_file_space_usage;\n\n-- PAGELATCH waits\nSELECT @PagelatchWaits = SUM(waiting_tasks_count)\nFROM sys.dm_os_wait_stats\nWHERE wait_type LIKE ''PAGELATCH_%'';\n\n-- Top session by TempDB usage\nSELECT TOP 1 \n    @TopSessionID = s.session_id,\n    @TopSessionCommand = r.command,\n    @TopSessionMBUsed = SUM(internal_objects_alloc_page_count + user_objects_alloc_page_count) * 8 / 1024\nFROM tempdb.sys.dm_db_task_space_usage t\nJOIN sys.dm_exec_sessions s ON t.session_id = s.session_id\nJOIN sys.dm_exec_requests r ON t.request_id = r.request_id\nGROUP BY s.session_id, r.command\nORDER BY SUM(internal_objects_alloc_page_count + user_objects_alloc_page_count) DESC;\n\n-- Insert log\nINSERT INTO dbo.TempDB_Monitor_Log\n(FileSizeMB, UsedSpaceMB, InternalObjectsMB, UserObjectsMB, VersionStoreMB, PagelatchWaits,\n TopSessionID, TopSessionCommand, TopSessionMBUsed)\nVALUES\n(@FileSizeMB, @UsedSpaceMB, @InternalMB, @UserMB, @VersionMB, @PagelatchWaits,\n @TopSessionID, @TopSessionCommand, @TopSessionMBUsed);\n',\n    @database_name = N'tempdb',\n    @on_success_action = 1;\n\n```\n>For most new allocations in user databases and tempdb, SQL Server now defaults to allocating uniform extents from the very beginning, even if the object only needs one page.\n\n>Mixed extents are still used for some internal purposes (like Index Allocation Map (IAM) pages) but are no longer the default initial allocation for user tables/indexes.\n\n### What is impact of NoLock\n* dirty reads; ignores nolocks, incositent read.\n\n### Reading Execution plan\n* estimated vs actual, costly operators (key lookup, sort, nested loop).\n* Analyze index usage, join types, and warning (missing stats)\n\n### Keyloopup vs RID lookup. \n* Keylookup: on the clustered table (seeks using clustered index)\n* RID lookup: on heap( row id fetch)\n* RID is more expensive due to heap access and less efficient page reads.\n### MAXDOP\n* It is maximum degree of parallelism. Total number of processor that a query can use to create thread and run the query. \n* Unlimited by default; can casuse cxpacket waits. skewed parallelism. Tuned based on CPU core, workload type (OLTP often set to 1 or 2)\n\n\n## Scenario-based Questions (TOP 5)\n### 2 AM, and the prod DB is down. Walk thorough your response.\n* check alert detials (monitoring system)\n* RDP into server; validate the service and the server itself.\n* Check error logs and Event Viewer\n* Check system resource (disk,cpu, memory, network)\n* Communicate status and recovery plan to the stakeholders\n* Start a incident ticket with vendor if enterprise db\n* Try bringing it up if its a known cause and can be resolved else\n    *  Initiate failover if in an HA setup\n* Document post-mortem in depth root cause and resolution for future reference and communicate with stakeholders.\n### Blame Game: Dev blames DB\n* Classic, Try to replicate the issue.\n* Collect all the metrics (execution plan, wait stats, resource usage)\n* present finding along with evidence and give suggestion if applicable\n* ask for collaborative review.\n* focus on join resolution.\n### impossible request: Real-time report on OLTP system.\n* Assess load impact and explain consequences.\n* Propose alternative (replication, read-only AG secondary, caching)\n* set expectations clearly.\n### Time you made a significant technical mistake\n* It was when i started the job was writing automation script for backups and failover maintainence or percona cluster mysql.\n* I ran the script with production .env file and as the script was on the test phase. \n* I ran the wrong directory structure and remove  root directory in linux of the server.\n* As it was Multi-master cluster with percona was all in sync. \n* Informed manager as it wasn't so raise the ticket with system adim to rebuild the server (VM).\n\n### Proactive improvement: 30-60-90 day plan.\n* Audit current state, implement basic monitoring, setup alerts if not. Check for the replication status, architecture. I believe understanding business is vital for DBA. Need to know what is critical and what is not. need to figure out which system needs load and which are free ones.\n* 60: Automated backups, index/stat maintencance, baseline metrics, Automated backups integrity and verification\n* 90: DR test plan, optimise high-cost queries, implement security and performance policy. \n\n\n\n\n",
    "category": "SQL Server Performance Tuning",
    "tags": [],
    "date": "2026-01-12",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "19 min read",
    "file": "Posts_Curated/sql-server-performance-tuning.md"
  },
  {
    "id": 12,
    "title": "OpenSearch PostgreSQL Dashboard: Complete Monitoring Setup with NDJSON",
    "excerpt": "A production-ready OpenSearch dashboard for monitoring PostgreSQL/Patroni clusters with real-time log analysis, error tracking, and interactive filters. Includes complete NDJSON for one-click import.",
    "content": "---\ntitle: \"OpenSearch PostgreSQL Dashboard: Complete Monitoring Setup with NDJSON\"\ndate: \"2026-01-15\"\ncategory: \"OpenSearch & Observability\"\ntags: [\"OpenSearch\", \"PostgreSQL\", \"Patroni\", \"Monitoring\", \"Dashboard\", \"Observability\", \"DevOps\"]\nexcerpt: \"A production-ready OpenSearch dashboard for monitoring PostgreSQL/Patroni clusters with real-time log analysis, error tracking, and interactive filters. Includes complete NDJSON for one-click import.\"\nauthor: \"Anish Karki\"\nfeatured: true\n---\n\n#  OpenSearch PostgreSQL & Patroni Monitoring Dashboard\n\nThis guide provides a complete, production-ready OpenSearch dashboard for monitoring PostgreSQL and Patroni cluster logs. The dashboard includes real-time visualizations, error tracking, and interactive filters for deep log analysis.\n\n##  What This Dashboard Provides\n\n| Feature | Description |\n|---------|-------------|\n| ** Real-time Metrics** | Total events, error counts, warning counts, active sources |\n| ** Time-Series Analysis** | Log volume over time, errors/warnings trends |\n| ** Interactive Filters** | Filter by hostname, log source, log type |\n| ** Node Comparison** | Compare Patroni1 vs Patroni2 log volumes |\n| ** Source Breakdown** | Visualize logs by source file |\n",
    "category": "OpenSearch & Observability",
    "tags": ["OpenSearch", "PostgreSQL", "Patroni", "Monitoring", "Dashboard", "Observability", "DevOps"],
    "date": "2026-01-15",
    "author": "Anish Karki",
    "featured": true,
    "readTime": "8 min read",
    "file": "Posts_Curated/opensearch-postgresql-dashboard.md"
  }
]